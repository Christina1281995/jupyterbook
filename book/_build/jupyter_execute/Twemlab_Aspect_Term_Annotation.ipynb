{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXCpOYodVc96",
    "tags": []
   },
   "source": [
    "# Annotating Twemlab Goldstandard Files to Include Aspect Term Labels\n",
    "\n",
    "The twemlab goldstandard files are already labelled according to emotions. In this notebook they are consolidated into the four basic emotions:\n",
    "- happiness (includes love and beauty) 🙂\n",
    "- anger (includes disgust) 😠\n",
    "- sadness 😞\n",
    "- fear 😨\n",
    "\n",
    "Aside from the consolidation of the emotion labels, this notebook uses helper functions to label each tweet according to its emotion-related aspect terms. The function splits the datasets into chunks of 100 tweets to save each annotated chunk during the process (for extra caution that the annotations are saved). Each tweet is shown to the user along with the labelled emotion. The user can enter how many aspect terms there are and then enters the aspect terms. The function then automatically identifies the beginning and inside of aspect terms and labels them according to the required input format of the GRACE model developed by [Luo et al. (2020)](https://arxiv.org/abs/2009.10557).\n",
    "\n",
    "For clarity and assistance during the labelling, \"aspect term\" need to be clearly defined.\n",
    "- asepcts are regarded as a \"general\" aspect, which collectively refers to an entity and its aspects as \"aspect\" ([Zhang et al., 2022](https://arxiv.org/pdf/2203.01054.pdf))\n",
    "- aspect term a is the opinion target which appears in the given text, e.g., “pizza” in the sentence “The pizza is delicious.” When the target is implicitly expressed (e.g., “It is overpriced!”), we can denote the aspect term as a special one named “null” ([Zhang et al., 2022](https://arxiv.org/pdf/2203.01054.pdf))\n",
    "- here only aspects are labelled that are related to the given emotion \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAzV-AQ1OmZW",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Required Fromat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-uBGgL4ngUN"
   },
   "source": [
    "For Training Step 1 'twitter_1_train.txt' file required:\n",
    "\n",
    "Reformat the two dataframes 'twemlab_birmingham' and 'twemlab_boston' to match the format below: \n",
    "\n",
    "\n",
    "```\n",
    "-DOCSTART-\n",
    "\n",
    "How - - O O O\n",
    "can - - O O O\n",
    "someone - - O O O\n",
    "so - - O O O\n",
    "incompetent - - O O O\n",
    "like - - O O O\n",
    "Maxine - - B_AP NEGATIVE B_AP+NEGATIVE\n",
    "Waters - - I_AP NEGATIVE I_AP+NEGATIVE\n",
    "stay - - O O O\n",
    "in - - O O O\n",
    "office - - O O O\n",
    "for - - O O O\n",
    "over - - O O O\n",
    "20 - - O O O\n",
    "years - - O O O\n",
    "? - - O O O\n",
    "#LAFail - - O O O\n",
    "\n",
    "@HabibaAlshanti - - B_AP POSITIVE B_AP+POSITIVE\n",
    ".. - - O O O\n",
    "Yes - - O O O\n",
    "I - - O O O\n",
    "want - - O O O\n",
    "that - - O O O\n",
    ":p - - O O O\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPWYSXEjVWBY",
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "t2kffv8yGiCa"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "import pandas as pd                                                    # data handling\n",
    "import xml.etree.cElementTree as ET \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQXV1f9YNJ01"
   },
   "source": [
    "### Load Both Goldstandard Files\n",
    "\n",
    "- Birmingham (994 tweets)\n",
    "- Boston (631 tweets)\n",
    "\n",
    "Load into dataframe --> match emotion labels (happiness, anger, sadness, fear) --> keep only id, text, emotion columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdWIhgZBNaD6",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Birmingham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltZoTDAnG2EJ",
    "outputId": "fdddae8c-5991-43bb-f18a-2aa8727936b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in goldstandard: 994\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000000000000001</td>\n",
       "      <td>who says summer is over; beautiful run in Edin...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200000000000000002</td>\n",
       "      <td>Eid prayer in small heath park 7:30am sharp to...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200000000000000003</td>\n",
       "      <td>did the last one at Summerfield Park</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200000000000000004</td>\n",
       "      <td>that was Summerfield Park</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200000000000000005</td>\n",
       "      <td>FREE led cycle ride from Edgbaston Reservoir, ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200000000000000006</td>\n",
       "      <td>The unnamed woman, in her 30s, had been on the...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200000000000000007</td>\n",
       "      <td>Attempt theft from Motor Vehicle - Rookery Par...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200000000000000008</td>\n",
       "      <td>Heather and webs... #heather #cobwebs #nature ...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200000000000000009</td>\n",
       "      <td>The unnamed woman, in her 30s, had been on the...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200000000000000010</td>\n",
       "      <td>Birmingham: Lianne Harris - mini Pype Hayes Pa...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200000000000000011</td>\n",
       "      <td>Pop-Up Storytelling, 6 Sep 2015: 6 Sep 2015 - ...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200000000000000012</td>\n",
       "      <td>sandwellcouncil   Kings Heath Park, Lickey Hil...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200000000000000013</td>\n",
       "      <td>The calm before the storm as we look to round ...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200000000000000014</td>\n",
       "      <td>Cotteridge park in the dark is dark.</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200000000000000015</td>\n",
       "      <td>Brill day exploring #prehistory w yr 3s  &amp; #br...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200000000000000016</td>\n",
       "      <td>Cannon Hill Park zinging with bats over the la...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200000000000000017</td>\n",
       "      <td>Vulnerable female found safe in Highbury park ...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200000000000000018</td>\n",
       "      <td>#Watercolour #painting from a photo of Sutton ...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>200000000000000019</td>\n",
       "      <td>Good luck all those competing in the ERRA Nati...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>200000000000000020</td>\n",
       "      <td>Shout out to all in National Road Relays actio...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0   200000000000000001  who says summer is over; beautiful run in Edin...   \n",
       "1   200000000000000002  Eid prayer in small heath park 7:30am sharp to...   \n",
       "2   200000000000000003               did the last one at Summerfield Park   \n",
       "3   200000000000000004                          that was Summerfield Park   \n",
       "4   200000000000000005  FREE led cycle ride from Edgbaston Reservoir, ...   \n",
       "5   200000000000000006  The unnamed woman, in her 30s, had been on the...   \n",
       "6   200000000000000007  Attempt theft from Motor Vehicle - Rookery Par...   \n",
       "7   200000000000000008  Heather and webs... #heather #cobwebs #nature ...   \n",
       "8   200000000000000009  The unnamed woman, in her 30s, had been on the...   \n",
       "9   200000000000000010  Birmingham: Lianne Harris - mini Pype Hayes Pa...   \n",
       "10  200000000000000011  Pop-Up Storytelling, 6 Sep 2015: 6 Sep 2015 - ...   \n",
       "11  200000000000000012  sandwellcouncil   Kings Heath Park, Lickey Hil...   \n",
       "12  200000000000000013  The calm before the storm as we look to round ...   \n",
       "13  200000000000000014               Cotteridge park in the dark is dark.   \n",
       "14  200000000000000015  Brill day exploring #prehistory w yr 3s  & #br...   \n",
       "15  200000000000000016  Cannon Hill Park zinging with bats over the la...   \n",
       "16  200000000000000017  Vulnerable female found safe in Highbury park ...   \n",
       "17  200000000000000018  #Watercolour #painting from a photo of Sutton ...   \n",
       "18  200000000000000019  Good luck all those competing in the ERRA Nati...   \n",
       "19  200000000000000020  Shout out to all in National Road Relays actio...   \n",
       "\n",
       "      emotion  \n",
       "0   happiness  \n",
       "1        none  \n",
       "2        none  \n",
       "3        none  \n",
       "4        none  \n",
       "5     sadness  \n",
       "6       anger  \n",
       "7   happiness  \n",
       "8     sadness  \n",
       "9     sadness  \n",
       "10  happiness  \n",
       "11       fear  \n",
       "12  happiness  \n",
       "13       fear  \n",
       "14  happiness  \n",
       "15  happiness  \n",
       "16  happiness  \n",
       "17  happiness  \n",
       "18  happiness  \n",
       "19  happiness  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load TwEmLab Goldstandard for Birmingham\n",
    "tree1 = ET.parse('../Data/twemlab_goldstandards_original/birmingham_labels.xml')\n",
    "root1 = tree1.getroot()\n",
    "\n",
    "# check contents\n",
    "#root1[0][1].text\n",
    "\n",
    "# create dataframe from xml file\n",
    "data1 = []\n",
    "for tweet in root1.findall('Tweet'):\n",
    "    id = tweet.find('ID').text\n",
    "    label = tweet.find('Label').text\n",
    "    data1.append((id, label))\n",
    "\n",
    "df1 = pd.DataFrame(data1,columns=['id','label'])\n",
    " # df1.head()\n",
    "    \n",
    "# Load TwEmLab Birmingham Tweets\n",
    "tree2 = ET.parse('../Data/twemlab_goldstandards_original/birmingham_tweets.xml')\n",
    "root2 = tree2.getroot()\n",
    "\n",
    "# check contents\n",
    "# root2[0][1].text\n",
    "\n",
    "# create dataframe from xml file\n",
    "data2 = []\n",
    "for tweet in root2.findall('Tweet'):\n",
    "    id = tweet.find('ID').text\n",
    "    text = tweet.find('text').text\n",
    "    goldstandard = tweet.attrib.get(\"goldstandard\")\n",
    "    data2.append((id, text, goldstandard))\n",
    "\n",
    "df2 = pd.DataFrame(data2,columns=['id','text', 'goldstandard'])\n",
    "# df2.head()\n",
    "\n",
    " # merge the two separate dataframes based on id columns\n",
    "merge = pd.merge(df1, df2, on='id')\n",
    "\n",
    "# keep only the tweets that are part of the goldstandard\n",
    "twemlab = merge[merge['goldstandard'] == 'yes']\n",
    "print(f'Number of tweets in goldstandard: {len(twemlab)}')\n",
    "\n",
    "emotions = []\n",
    "# assign emotion label (happiness, anger, sadness, fear)\n",
    "for index, row in twemlab.iterrows():\n",
    "    if row['label'] == 'beauty' or row['label'] == 'happiness':\n",
    "        emotions.append('happiness')\n",
    "    elif row['label'] == 'anger/disgust':\n",
    "        emotions.append('anger')\n",
    "    elif row['label'] == 'sadness':\n",
    "        emotions.append('sadness')\n",
    "    elif row['label'] == 'fear':\n",
    "        emotions.append('fear')\n",
    "    else: \n",
    "        emotions.append('none')\n",
    "        \n",
    "twemlab['emotion'] = emotions\n",
    "\n",
    "twemlab_birmingham = twemlab[['id','text','emotion']]\n",
    "\n",
    "# check dataset\n",
    "twemlab_birmingham.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PRJOpw0NfbD",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hrl9PcKvMUFh",
    "outputId": "d50270c0-c2b6-4aa1-c995-2f0c641a4026"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_6284\\1233096741.py:1: DtypeWarning: Columns (2,4,5,8,19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  readfile = pd.read_csv('../Data/twemlab_goldstandards_original/boston_goldstandard.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Tweet_timestamp</th>\n",
       "      <th>Tweet_text</th>\n",
       "      <th>Tweet_goldstandard_attribute</th>\n",
       "      <th>Tweet_longitude</th>\n",
       "      <th>Tweet_latitude</th>\n",
       "      <th>Tweet_timestamp</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.232433e+17</td>\n",
       "      <td>14/04/2013 01:16</td>\n",
       "      <td>yeah, well if ur with family all the time I se...</td>\n",
       "      <td>yes</td>\n",
       "      <td>-70.881896</td>\n",
       "      <td>42.298795</td>\n",
       "      <td>14/04/2013 01:16</td>\n",
       "      <td>anger/disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.210770e+17</td>\n",
       "      <td>08/04/2013 01:48</td>\n",
       "      <td>He kiss that ass #i #aint #wit #that #shit !</td>\n",
       "      <td>yes</td>\n",
       "      <td>-71.075771</td>\n",
       "      <td>42.319011</td>\n",
       "      <td>08/04/2013 01:48</td>\n",
       "      <td>anger/disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.211332e+17</td>\n",
       "      <td>08/04/2013 05:31</td>\n",
       "      <td>Guy next to me on the flight has those shoes t...</td>\n",
       "      <td>yes</td>\n",
       "      <td>-71.076238</td>\n",
       "      <td>42.350379</td>\n",
       "      <td>08/04/2013 05:31</td>\n",
       "      <td>anger/disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.228748e+17</td>\n",
       "      <td>13/04/2013 00:51</td>\n",
       "      <td>never!!!</td>\n",
       "      <td>yes</td>\n",
       "      <td>-71.044951</td>\n",
       "      <td>42.334131</td>\n",
       "      <td>13/04/2013 00:51</td>\n",
       "      <td>anger/disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.230403e+17</td>\n",
       "      <td>13/04/2013 11:49</td>\n",
       "      <td>I'm gettin really sick &amp; tired of stepping in ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>-71.142320</td>\n",
       "      <td>42.346204</td>\n",
       "      <td>13/04/2013 11:49</td>\n",
       "      <td>anger/disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tweet_ID   Tweet_timestamp  \\\n",
       "0  3.232433e+17  14/04/2013 01:16   \n",
       "1  3.210770e+17  08/04/2013 01:48   \n",
       "2  3.211332e+17  08/04/2013 05:31   \n",
       "3  3.228748e+17  13/04/2013 00:51   \n",
       "4  3.230403e+17  13/04/2013 11:49   \n",
       "\n",
       "                                          Tweet_text  \\\n",
       "0  yeah, well if ur with family all the time I se...   \n",
       "1       He kiss that ass #i #aint #wit #that #shit !   \n",
       "2  Guy next to me on the flight has those shoes t...   \n",
       "3                                           never!!!   \n",
       "4  I'm gettin really sick & tired of stepping in ...   \n",
       "\n",
       "  Tweet_goldstandard_attribute  Tweet_longitude  Tweet_latitude  \\\n",
       "0                          yes       -70.881896       42.298795   \n",
       "1                          yes       -71.075771       42.319011   \n",
       "2                          yes       -71.076238       42.350379   \n",
       "3                          yes       -71.044951       42.334131   \n",
       "4                          yes       -71.142320       42.346204   \n",
       "\n",
       "    Tweet_timestamp        Emotion  \n",
       "0  14/04/2013 01:16  anger/disgust  \n",
       "1  08/04/2013 01:48  anger/disgust  \n",
       "2  08/04/2013 05:31  anger/disgust  \n",
       "3  13/04/2013 00:51  anger/disgust  \n",
       "4  13/04/2013 11:49  anger/disgust  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readfile = pd.read_csv('../Data/twemlab_goldstandards_original/boston_goldstandard.csv')\n",
    "twemlab_boston = readfile[['Tweet_ID', 'Tweet_timestamp', 'Tweet_text', 'Tweet_goldstandard_attribute', 'Tweet_longitude','Tweet_latitude','Tweet_timestamp','Emotion']]\n",
    "# use only rows that have text in them\n",
    "twemlab_boston = twemlab_boston[0:631]\n",
    "twemlab_boston.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqLpneKKNzId",
    "outputId": "b62b7b6a-95e5-4230-a2b2-fdb58fe98a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n"
     ]
    }
   ],
   "source": [
    "print(len(twemlab_birmingham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bpp5IIWPG2Ba",
    "outputId": "a318101c-b389-4400-83fc-7b7e0af5694a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Tweet_text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.232433e+17</td>\n",
       "      <td>yeah, well if ur with family all the time I se...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.210770e+17</td>\n",
       "      <td>He kiss that ass #i #aint #wit #that #shit !</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.211332e+17</td>\n",
       "      <td>Guy next to me on the flight has those shoes t...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.228748e+17</td>\n",
       "      <td>never!!!</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.230403e+17</td>\n",
       "      <td>I'm gettin really sick &amp; tired of stepping in ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.245137e+17</td>\n",
       "      <td>Hey can you stop making out in the seat in fro...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.252593e+17</td>\n",
       "      <td>That just pissed me off -_-</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.213107e+17</td>\n",
       "      <td>All of #boston is laying mulch today. The enti...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.248940e+17</td>\n",
       "      <td>â€œ: Obama is really not thinking at the momen...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.259976e+17</td>\n",
       "      <td>Tan bruto (/.-) RT \":  noooooooooooooooooooooo...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.212604e+17</td>\n",
       "      <td>The fact there is an elderly lady and pregnant...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.213831e+17</td>\n",
       "      <td>How do you feel? I just shit on your life? Ha</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.210718e+17</td>\n",
       "      <td>The fact my mom is being an ass and pointing o...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.217203e+17</td>\n",
       "      <td>OMG biggest waste of my time.</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.261652e+17</td>\n",
       "      <td>Shit man! Wtf did I do this time for you to act?</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.234615e+17</td>\n",
       "      <td>I can't stand the homework anymore , semester ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.210783e+17</td>\n",
       "      <td>S/o to Pujols, fighting through that injury to...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.242897e+17</td>\n",
       "      <td>Rtâ€œ: Sick of people claiming they woke up in...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.248830e+17</td>\n",
       "      <td>We wonder why our world marinates in fucking d...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.252356e+17</td>\n",
       "      <td>are you shitting me right now. Wtf</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tweet_ID                                         Tweet_text emotion\n",
       "0   3.232433e+17  yeah, well if ur with family all the time I se...   anger\n",
       "1   3.210770e+17       He kiss that ass #i #aint #wit #that #shit !   anger\n",
       "2   3.211332e+17  Guy next to me on the flight has those shoes t...   anger\n",
       "3   3.228748e+17                                           never!!!   anger\n",
       "4   3.230403e+17  I'm gettin really sick & tired of stepping in ...   anger\n",
       "5   3.245137e+17  Hey can you stop making out in the seat in fro...   anger\n",
       "6   3.252593e+17                        That just pissed me off -_-   anger\n",
       "7   3.213107e+17  All of #boston is laying mulch today. The enti...   anger\n",
       "8   3.248940e+17  â€œ: Obama is really not thinking at the momen...   anger\n",
       "9   3.259976e+17  Tan bruto (/.-) RT \":  noooooooooooooooooooooo...   anger\n",
       "10  3.212604e+17  The fact there is an elderly lady and pregnant...   anger\n",
       "11  3.213831e+17      How do you feel? I just shit on your life? Ha   anger\n",
       "12  3.210718e+17  The fact my mom is being an ass and pointing o...   anger\n",
       "13  3.217203e+17                      OMG biggest waste of my time.   anger\n",
       "14  3.261652e+17   Shit man! Wtf did I do this time for you to act?   anger\n",
       "15  3.234615e+17  I can't stand the homework anymore , semester ...   anger\n",
       "16  3.210783e+17  S/o to Pujols, fighting through that injury to...   anger\n",
       "17  3.242897e+17  Rtâ€œ: Sick of people claiming they woke up in...   anger\n",
       "18  3.248830e+17  We wonder why our world marinates in fucking d...   anger\n",
       "19  3.252356e+17                 are you shitting me right now. Wtf   anger"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = []\n",
    "# assign emotion label (happiness, anger, sadness, fear)\n",
    "for index, row in twemlab_boston.iterrows():\n",
    "    if row['Emotion'] == 'beauty' or row['Emotion'] == 'happiness':\n",
    "        emotions.append('happiness')\n",
    "    elif row['Emotion'] == 'anger/disgust':\n",
    "        emotions.append('anger')\n",
    "    elif row['Emotion'] == 'sadness':\n",
    "        emotions.append('sadness')\n",
    "    elif row['Emotion'] == 'fear':\n",
    "        emotions.append('fear')\n",
    "    else: \n",
    "        emotions.append('none')\n",
    "        \n",
    "twemlab_boston['emotion'] = emotions\n",
    "\n",
    "twemlab_boston = twemlab_boston[['Tweet_ID','Tweet_text','emotion']]\n",
    "\n",
    "# check dataset\n",
    "twemlab_boston.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izFQFjbAm771",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Helper Functions for Aspect Term Labelling and Reformatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kzZW8tO1iMnj"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def splitdataset(df, chunksize):\n",
    "\n",
    "  # split up the twemlab goldstandard texts into 100 tweet chunks\n",
    "  nr_of_iterations = (math.ceil(len(df) / chunksize))\n",
    "\n",
    "  print(f'{nr_of_iterations} subsets created from the whole dataframe.\\n')\n",
    "\n",
    "  list_of_chunks = []\n",
    "\n",
    "  for a in range(nr_of_iterations):\n",
    "    cur_index = a*chunksize\n",
    "    if a == nr_of_iterations:\n",
    "      chunk_of_df = df[cur_index: len(df)]\n",
    "\n",
    "    else: \n",
    "      chunk_of_df = df[cur_index: cur_index+chunksize]\n",
    "\n",
    "    list_of_chunks.append(chunk_of_df)\n",
    "\n",
    "  return list_of_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XM0FsB15G1-7"
   },
   "outputs": [],
   "source": [
    "def annotate_aspects(df, text_col, emotions_col):\n",
    "\n",
    "  # text column to list\n",
    "  text_list = list(df[text_col])\n",
    "  emotion_list = list(df[emotions_col])\n",
    "\n",
    "  # addition to words that aren't aspect terms\n",
    "  addition_no_ate = ' - - O O O'\n",
    "\n",
    "  # function to use for beginning of aspect terms\n",
    "  def beginning_ate(emotion):\n",
    "    addition_beginning_ate = ' - - B_AP ' + emotion + ' B_AP+' + emotion\n",
    "    return addition_beginning_ate\n",
    "\n",
    "  # function to use for inside aspect terms\n",
    "  def inside_ate(emotion):\n",
    "    inside_beginning_ate = ' - - I_AP ' + emotion + ' I_AP+' + emotion\n",
    "    return inside_beginning_ate\n",
    "\n",
    "  # list to store all reformatted tweets\n",
    "  convert_to_doc = []\n",
    "\n",
    "  # logic to iteratively work in chunks\n",
    "  counter_overall = 0\n",
    "\n",
    "  # iteratively apply re-formatting and save to new list\n",
    "  while counter_overall != len(text_list):\n",
    "\n",
    "    # print a line to separate \n",
    "    print(\"---------------------------------------\")    \n",
    "    print(f'Tweet nr:  {counter_overall}')\n",
    "\n",
    "    words = re.findall(r\"[#\\w\\-]+|[.,!?():;\\\"\\']\", text_list[counter_overall])\n",
    "     \n",
    "    # show me the emotion\n",
    "    # print(f\"Emotion:   {emotion_list[counter_overall]}\")\n",
    "    \n",
    "    # show me the text and let me determine how many aspects there are\n",
    "    how_many_aspect_words = input(f\"Emotion:   {emotion_list[counter_overall]} ------------ Tweet:     {' '.join(words)} ------------ How many aspect words?\")\n",
    "    # check that input is correct, otherwise prompt again until input is digit\n",
    "    while how_many_aspect_words.isdigit() == False:\n",
    "      ask_again = input(f\"Emotion:   {emotion_list[counter_overall]} ------------ Tweet:     {' '.join(words)} ------------ How many aspect words? Enter a number:\")\n",
    "      if ask_again.isdigit():\n",
    "        how_many_aspect_words = ask_again\n",
    "    \n",
    "    #list to collect aspect terms\n",
    "    aspect_terms = []\n",
    "\n",
    "    # list for entire reformatted tweet\n",
    "    new_tweet = []\n",
    "\n",
    "    # for each aspect phrase, enter the word to add it to aspect terms list\n",
    "    for i in range(int(how_many_aspect_words)):\n",
    "\n",
    "      get_word = input(f\"Emotion:   {emotion_list[counter_overall]} ------------ Tweet:     {' '.join(words)} ------------ Aspect word {i}:\")\n",
    "      aspect_terms.append(get_word)\n",
    "      #print(aspect_terms)\n",
    "\n",
    "    # for each word that has been added to the list, when it is found add spectial annotions\n",
    "    for j in range(len(words)):\n",
    "      # check if the word is in the aspect terms list\n",
    "      if words[j] in aspect_terms:\n",
    "        # if its the first word of the tweet, no need to check if there's an aspect word before\n",
    "        if j == 0:\n",
    "          add_to_doc = words[j] + beginning_ate(emotion_list[counter_overall])\n",
    "        elif j > 0:\n",
    "          # if there's an aspect term before this word\n",
    "          if words[j-1] in aspect_terms:\n",
    "            add_to_doc = words[j] + inside_ate(emotion_list[counter_overall])\n",
    "          else:\n",
    "            add_to_doc = words[j] + beginning_ate(emotion_list[counter_overall])\n",
    "\n",
    "      elif j not in aspect_terms:\n",
    "        add_to_doc = words[j] + addition_no_ate\n",
    "\n",
    "      new_tweet.append(add_to_doc)\n",
    "    \n",
    "    #print(new_tweet)\n",
    "    save = input(f'{new_tweet} ------------ Save last reformatted text? Enter/n:')\n",
    "    if save == '':\n",
    "      convert_to_doc.append(new_tweet)\n",
    "    if save == 'n':\n",
    "      # set counter back and don't do anything with the tweet\n",
    "      counter_overall -= 1\n",
    "      print(f'counter overall set back to : {counter_overall}')\n",
    "    \n",
    "    counter_overall += 1\n",
    "\n",
    "  return convert_to_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hqTjnaQnT8E",
    "tags": []
   },
   "source": [
    "### Annotate\n",
    "\n",
    "Subdivide the datasets into subsets and iterate over them to identify asepct terms, reformat and store in variable for later conversion into a .txt file.\n",
    "\n",
    "This step requires iteratively uncommenting the individual subsets below to annotate them one after the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hMImONJOvwWh"
   },
   "outputs": [],
   "source": [
    "# choose and keep track of how many already annotated\n",
    "#testing = twemlab_birmingham[80:90]\n",
    "#print(boston_100.loc[99])\n",
    "\n",
    "all_docs = []\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split whole dataframe into subsets of 100 tweets each\n",
    "# twemlab_birmingham\n",
    "# twemlab_boston\n",
    "\n",
    "# choose and keep track of how many already annotated\n",
    "#boston = twemlab_boston[0:100]             # done\n",
    "#boston = twemlab_boston[100:200]           # done\n",
    "#boston = twemlab_boston[200:300]           # done\n",
    "#boston = twemlab_boston[300:400]           # done\n",
    "#boston = twemlab_boston[400:500]           # done\n",
    "#boston = twemlab_boston[500:600]           # done\n",
    "#boston = twemlab_boston[600:]              # done\n",
    "\n",
    "# birmingham\n",
    "#birmingham = twemlab_birmingham[0:100]      # done\n",
    "#birmingham = twemlab_birmingham[100:200]    # done\n",
    "#birmingham = twemlab_birmingham[200:300]    # done\n",
    "#birmingham = twemlab_birmingham[300:400]    # done\n",
    "#birmingham = twemlab_birmingham[400:500]    # done\n",
    "\n",
    "#birmingham = twemlab_birmingham[500:600]    # done\n",
    "#birmingham = twemlab_birmingham[600:700]    # done\n",
    "#birmingham = twemlab_birmingham[700:800]    # done\n",
    "#birmingham = twemlab_birmingham[800:900]    # done\n",
    "#birmingham = twemlab_birmingham[900:]       # done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(all_docs))\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'birmingham' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# split into subsets of 20 tweets that will be saved on the go\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m list_of_chunks \u001b[38;5;241m=\u001b[39m splitdataset(\u001b[43mbirmingham\u001b[49m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subset \u001b[38;5;129;01min\u001b[39;00m list_of_chunks:\n\u001b[0;32m      5\u001b[0m     convert_to_doc \u001b[38;5;241m=\u001b[39m annotate_aspects(df\u001b[38;5;241m=\u001b[39msubset, text_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, emotions_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'birmingham' is not defined"
     ]
    }
   ],
   "source": [
    "# split into subsets of 20 tweets that will be saved on the go\n",
    "list_of_chunks = splitdataset(birmingham, 10)\n",
    "\n",
    "for subset in list_of_chunks:\n",
    "    convert_to_doc = annotate_aspects(df=subset, text_col='text', emotions_col='emotion')\n",
    "    #convert_to_doc = annotate_aspects(df=subset, text_col='Tweet_text', emotions_col='emotion')\n",
    "\n",
    "    # convert this chunk to a separate txt file to save progress up to here\n",
    "  \n",
    "    # keep track of how many tweets are going into the final reforatted document\n",
    " \n",
    "    with open(f\"../Data/twemlab_goldstandards_annotated_reformatted/subsets/re-formatted_birmingham_subset_{i}.txt\", mode = \"w\") as f:\n",
    "        for tweet in convert_to_doc:\n",
    "            for word in tweet:\n",
    "                f.write(\"%s\\n\" % word)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"\\n----------------\\nSubset {i} was reformatted and saved into Data/twemlab_goldstandards_annotated_reformatted/subsets/re-formatted_birmingham_subset_{i}.txt\\n----------------\\n\")\n",
    "\n",
    "    # add all to a list to be converted to a single output document\n",
    "    all_docs.append(convert_to_doc)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMAiVI6knlHR",
    "tags": []
   },
   "source": [
    "### Save all subsets together as new txt file\n",
    "\n",
    "This step ONLY works if the variable \"all_docs\" from the previous step contains the labelled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCxZx1A_G10x",
    "outputId": "e74e82a1-2a67-4e44-b6f2-50bc9990f1e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493 tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt2_annotated_reformatted.txt\n",
      "443 tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt2_train.txt\n",
      "50 tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/bimingham_pt2_test.txt\n"
     ]
    }
   ],
   "source": [
    "# ALL TWEETS\n",
    "tracker = 0\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt1_annotated_reformatted.txt\", mode = \"w\") as f:\n",
    "with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt2_annotated_reformatted.txt\", mode = \"w\") as f:\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/boston_annotated_reformatted.txt\", mode = \"w\") as f:\n",
    "    f.write(\"-DOCSTART-\\n\\n\")\n",
    "    for doc in all_docs:\n",
    "        for tweet in doc:\n",
    "            #every 10th tweet\n",
    "            for word in tweet:\n",
    "                f.write(\"%s\\n\" % word)\n",
    "            f.write(\"\\n\")\n",
    "            tracker += 1\n",
    "            \n",
    "#print(f\"{tracker} tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt1_annotated_reformatted.txt\")\n",
    "print(f\"{tracker} tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt2_annotated_reformatted.txt\")\n",
    "#print(f\"{tracker} tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/boston_annotated_reformatted.txt\")\n",
    "\n",
    "# TRAIN FILE\n",
    "\n",
    "tracker = 0\n",
    "iterator = 0\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt1_train.txt\", mode = \"w\") as f:\n",
    "with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt2_train.txt\", mode = \"w\") as f:\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/boston_train.txt\", mode = \"w\") as f:\n",
    "    f.write(\"-DOCSTART-\\n\\n\")\n",
    "    for doc in all_docs:\n",
    "        for tweet in doc:\n",
    "            #every 10th tweet\n",
    "            if iterator % 10 != 0:\n",
    "                for word in tweet:\n",
    "                    f.write(\"%s\\n\" % word)\n",
    "                f.write(\"\\n\")\n",
    "                tracker += 1\n",
    "            iterator += 1\n",
    "\n",
    "#print(f\"{tracker} tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt1_train.txt\")\n",
    "print(f\"{tracker} tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt2_train.txt\")\n",
    "#print(f\"{tracker} tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/boston_train.txt\")\n",
    "\n",
    "# TEST FILE\n",
    "\n",
    "tracker = 0\n",
    "iterator = 0\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt1_test.txt\", mode = \"w\") as f:\n",
    "with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt2_test.txt\", mode = \"w\") as f:\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/boston_test.txt\", mode = \"w\") as f:\n",
    "    f.write(\"-DOCSTART-\\n\\n\")\n",
    "    for doc in all_docs:\n",
    "        for tweet in doc:\n",
    "            #every 10th tweet\n",
    "            if iterator % 10 == 0:\n",
    "                for word in tweet:\n",
    "                    f.write(\"%s\\n\" % word)\n",
    "                f.write(\"\\n\")\n",
    "                tracker += 1\n",
    "            iterator += 1\n",
    "\n",
    "#print(f\"{tracker} tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/bimingham_pt1_test.txt\")\n",
    "print(f\"{tracker} tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/bimingham_pt2_test.txt\")\n",
    "#print(f\"{tracker} tweets were reformatted into Data/twemlab_goldstandards_annotated_reformatted/boston_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Datasets into Single Files\n",
    "\n",
    "Final Files:\n",
    "- Twemlab_all_annotated_reformatted.txt (100% of all data)\n",
    "- Twemlab_all_test.txt (10% of all data)\n",
    "- Twemlab_all_train.txt (90% of all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty variables\n",
    "d1 = d2 = d3 = \"\"\n",
    "\n",
    "# read in the previous 3 files (boston reformatted, brimingham pt 1 reformatted, birmingham pt 2 reformatted)\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/boston_annotated_reformatted.txt\") as fp:\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/boston_test.txt\") as fp:\n",
    "with open(\"../Data/twemlab_goldstandards_annotated_reformatted/boston_train.txt\") as fp:\n",
    "    d1 = fp.read()\n",
    "\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt1_annotated_reformatted.txt\") as fp:\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt1_test.txt\") as fp:\n",
    "with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt1_train.txt\") as fp:\n",
    "    d2 = fp.read()\n",
    "\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt2_annotated_reformatted.txt\") as fp:\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt2_test.txt\") as fp:\n",
    "with open(\"../Data/twemlab_goldstandards_annotated_reformatted/birmingham_pt2_train.txt\") as fp:\n",
    "    d3 = fp.read()\n",
    "\n",
    "# merge files\n",
    "data = d1 +  d2 + d3 \n",
    "\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/twemlab_all_annotated_reformatted.txt\", \"w\") as fp:\n",
    "#with open(\"../Data/twemlab_goldstandards_annotated_reformatted/twemlab_all_test.txt\", \"w\") as fp:\n",
    "with open(\"../Data/twemlab_goldstandards_annotated_reformatted/twemlab_all_train.txt\", \"w\") as fp:\n",
    "    fp.write(data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "XAzV-AQ1OmZW",
    "sPWYSXEjVWBY",
    "kdWIhgZBNaD6",
    "7PRJOpw0NfbD",
    "izFQFjbAm771"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.15 ('GRACE_Model')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "35d66ca9f48e1a2d5604fdbf7edac7b2f593e1efb3071f679b8d86788377cffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}