
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Aspect-Based Sentiment Analysis on Twitter Data &#8212; Sentiment Analysis</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Aspect-Based Emotion Analysis on Twitter Data" href="GRACE_trained_for_ABEA_on_twitter_data.html" />
    <link rel="prev" title="Document Level Sentiment Analysis with Deep Learning Models" href="Deep_Learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo1.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Sentiment Analysis</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Overview
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction_to_SA.html">
   Introduction to Sentiment Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Lexicon_and_Machine_Learning.html">
   Document-Level Sentiment Analysis with Lexicon and Machine Learning Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Deep_Learning.html">
   Document Level Sentiment Analysis with Deep Learning Models
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Aspect-Based Sentiment Analysis on Twitter Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="GRACE_trained_for_ABEA_on_twitter_data.html">
   Aspect-Based Emotion Analysis on Twitter Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Twemlab_Aspect_Term_Annotation.html">
   Annotating Twemlab Goldstandard Files to Include Aspect Term Labels
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Datasets.html">
   Datasets for Sentiment Analysis and Aspect-Based Sentiment Analysis
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/GRACE_trained_for_ABSA_on_twitter_data.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FGRACE_trained_for_ABSA_on_twitter_data.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/GRACE_trained_for_ABSA_on_twitter_data.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-tasks-of-absa">
   The Tasks of ABSA
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#comparing-published-methodologies">
   Comparing published methodologies
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grace-gradient-harmonized-and-cascaded-labeling-for-aspect-based-sentiment-analysis">
   GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-Based Sentiment Analysis
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grace-model-architecture-and-characteristics">
     GRACE Model Architecture and Characteristics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grace-model-setup-loading-from-last-training-step-and-epoch">
     GRACE Model Setup (Loading from last training Step and Epoch)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apply-model-on-twemlab-goldstandard-data">
     Apply Model on Twemlab Goldstandard Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apply-model-on-aifer-twitter-dataset">
     Apply Model on AIFER Twitter Dataset
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Aspect-Based Sentiment Analysis on Twitter Data</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-tasks-of-absa">
   The Tasks of ABSA
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#comparing-published-methodologies">
   Comparing published methodologies
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grace-gradient-harmonized-and-cascaded-labeling-for-aspect-based-sentiment-analysis">
   GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-Based Sentiment Analysis
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grace-model-architecture-and-characteristics">
     GRACE Model Architecture and Characteristics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grace-model-setup-loading-from-last-training-step-and-epoch">
     GRACE Model Setup (Loading from last training Step and Epoch)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apply-model-on-twemlab-goldstandard-data">
     Apply Model on Twemlab Goldstandard Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apply-model-on-aifer-twitter-dataset">
     Apply Model on AIFER Twitter Dataset
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="aspect-based-sentiment-analysis-on-twitter-data">
<h1>Aspect-Based Sentiment Analysis on Twitter Data<a class="headerlink" href="#aspect-based-sentiment-analysis-on-twitter-data" title="Permalink to this headline">#</a></h1>
<img align='right' src='https://git.sbg.ac.at/s1080384/sentimentanalysis/-/wikis/uploads/8729054731d3bf4f16e1d5a0b2fa23f5/absa.png' alt='An example of the four key sentiment elements of ABSA. (Zhang et al., 2022)' width='50%'>
<p>Recently, a more fine-grained “aspect-based sentiment analysis” (short ABSA) has been taking the lead from the traditional sentence and document-level sentiment classification. It’s popularity has been fuelled by the increasing capabilities of deep learning models and the lacking granularity in convencional document-level sentiment analysis. ABSA <strong>considers text data at the token level</strong>, whereas document-level analyses are based on the assumption that the document is concerned with a single topic, which often proves untrue.</p>
<p>Zhang et al. (2022) summarize this trend towards more fine-grained analysis levels:</p>
<blockquote>
<div><p><em>In the ABSA problem, the concerned target on which the sentiment is expressed shifts from an entire document to an entity or a certain aspect of an entity.</em></p>
</div></blockquote>
<p>The term “aspect” can generally refer to both the aspect of an entity as well as a special “general” aspect. Hence, it is often used to collectively refer to the entity or an entity’s aspect.</p>
<p><strong>ABSA encompasses</strong> the identification of one or more of four sentiment elements. Depending on the goal researchers set, Zhang et al. (2022) divide their ABSA methods into either <strong>Single ABSA</strong> tasks (the more conventional method for ABSA, where a method is developed to tackle one single ABSA goal) or <strong>Compound ABSA</strong> tasks (more recent trends have moved towards developing methods that address two or more sentiment goals in a single method, thereby capturing the dependency between them).</p>
<ul class="simple">
<li><p><strong>aspect category</strong> <em>c</em> defines a unique aspect of an entity and is supposed to fall into a category set C, predefined for each specific domain of interest. For example, <code class="docutils literal notranslate"><span class="pre">food</span></code> and <code class="docutils literal notranslate"><span class="pre">service</span></code> can be aspect categories for the restaurant domain.</p></li>
<li><p><strong>aspect term</strong> <em>a</em> is the opinion target which explicitly appears in the given text, e.g., <code class="docutils literal notranslate"><span class="pre">“pizza”</span></code> in the sentence “The pizza is delicious.” When the target is implicitly expressed (e.g., “It is overpriced!”), we denote the aspect term as a special one named “null”.</p></li>
<li><p><strong>opinion term</strong> <em>o</em> is the expression given by the opinion holder to express his/her sentiment towards the target. For instance, <code class="docutils literal notranslate"><span class="pre">“delicious”</span></code> is the opinion term in the running example “The pizza is delicious”.</p></li>
<li><p><strong>sentiment polarity</strong> <em>p</em> describes the orientation of the sentiment over an aspect category or an aspect term, which usually includes <code class="docutils literal notranslate"><span class="pre">positive</span></code>, <code class="docutils literal notranslate"><span class="pre">negative</span></code>, and <code class="docutils literal notranslate"><span class="pre">neutral</span></code>.</p></li>
</ul>
<br>
<section id="the-tasks-of-absa">
<h2>The Tasks of ABSA<a class="headerlink" href="#the-tasks-of-absa" title="Permalink to this headline">#</a></h2>
<img src="https://github.com/Christina1281995/demo-repo/blob/main/absatasks.png?raw=true" width="50%" align="right">
<p>Based on the comprehensive and recent overview provided by Zhang et al. (2022), several methods were systematically identified for further investigation:
Potential tasks of interest:</p>
<ul class="simple">
<li><p>ATE (aspect term extraction)</p></li>
<li><p>ASC (aspect sentiment classification)</p></li>
<li><p>E2E (end 2 end, ATE + ASC)</p></li>
<li><p>ASTE (aspect sentiment triple extraction, ATE, OTE, ASC)</p></li>
</ul>
<p>Either a pipeline method consisting of an ATE and ASC methods, or one of the compound methods may suffice for ABSA on tweet data.
All 4 above mentioned potential tasks were investigated concerning their documented performance on well-known datasets (mostly SemEval 2014, 2015, 2016 but also Mitchell et al.’s 2013 twitter dataset). The top scoring methods for each task are selected for more in-depth inspection. Priority was also given to methods that scored particularly well on the twitter dataset.</p>
<img src="https://git.sbg.ac.at/geo-social-analytics/geo-social-media/sentiment-analyses/uploads/a23ed12a3d3860816556bcf159a7adf2/methods_of_general_interest.png" width="80%">
<br>
<hr>
<br>
</section>
<section id="comparing-published-methodologies">
<h2>Comparing published methodologies<a class="headerlink" href="#comparing-published-methodologies" title="Permalink to this headline">#</a></h2>
<details><summary>ATE Methods</summary>
<img src="https://git.sbg.ac.at/geo-social-analytics/geo-social-media/sentiment-analyses/uploads/380415e631882f5cee2aba0c57eb4be3/image.png">
</details>
<details><summary>ASC Methods</summary>
<img src="https://git.sbg.ac.at/geo-social-analytics/geo-social-media/sentiment-analyses/uploads/91e300023dfe90e3c37a1a5dbc20b65b/image.png">
</details>
<details><summary>End 2 End Methods</summary>
<img src="https://git.sbg.ac.at/geo-social-analytics/geo-social-media/sentiment-analyses/uploads/5ab513146d2f580dc2dc5609c04e2950/image.png">
</details>
<details><summary>ASTE Methods</summary>
<img src="https://git.sbg.ac.at/geo-social-analytics/geo-social-media/sentiment-analyses/uploads/20a20dc8168d41741d32912887476858/image.png">
</details>
<br>
<hr>
</section>
<section id="grace-gradient-harmonized-and-cascaded-labeling-for-aspect-based-sentiment-analysis">
<h2>GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-Based Sentiment Analysis<a class="headerlink" href="#grace-gradient-harmonized-and-cascaded-labeling-for-aspect-based-sentiment-analysis" title="Permalink to this headline">#</a></h2>
<p>The method designed by Luo et al. (2020), described in the paper <a class="reference external" href="https://arxiv.org/abs/2009.10557">GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-based Sentiment Analysis. Huaishao Luo, Lei Ji, Tianrui Li, Nan Duan, Daxin Jiang. Findings of EMNLP, 2020.</a>, implements a gradient harmonized and cascaded labeling model.</p>
<p>The method falls into the “End 2 End” category of aspect-based sentiment analysis tasks, meaning it solves two ABSA sub-tasks, ATE (asect term extraction) and ASC (aspect semtiment classification), in one model or methodology. Recent advances in the E2E methods leverage the interdependencies between aspect term detection and its sentiment classification to enhance model performances. This stands in contrast to pipeline approaches, which tackle one ABSA sub-task after the other in an isolated manner.</p>
<img src='https://github.com/ArrowLuo/GRACE/raw/master/accessory/Framework.png'>
<br>
<br>
<section id="grace-model-architecture-and-characteristics">
<h3>GRACE Model Architecture and Characteristics<a class="headerlink" href="#grace-model-architecture-and-characteristics" title="Permalink to this headline">#</a></h3>
<img src="https://github.com/Christina1281995/demo-repo/blob/main/joint.PNG?raw=true" align="right" width="40%">
<ul class="simple">
<li><p>Co-extraction of ATE and ASC</p></li>
<li><p>2 cascading modules</p>
<ul>
<li><p>12 stacked transformer encoder blocks for ATE</p></li>
<li><p>3 shared transformer encoder blocks and 2 transformer decoder blocks for ASC</p></li>
</ul>
</li>
<li><p>Focus on interaction</p></li>
<li><p>Joint approach</p></li>
<li><p>Shared shallow layers (n=3)</p>
<ul>
<li><p>higher layers in BERT are usually task-specific</p></li>
<li><p>it is assumed that can be useful to share the shallow layers</p></li>
<li><p>generates a shared “baseline understanding”
<img src="https://github.com/Christina1281995/demo-repo/blob/main/grad.PNG?raw=true" align="right" width="40%"></p></li>
</ul>
</li>
<li><p><strong>Virtual adversairal training</strong>: the robustness of the model is improved bz preturbing the input data in small ways so that its difficult for the model to classify (to implement this, the direction and distance of the perturbations is calculated)</p></li>
<li><p><strong>Gradient harmonized loss</strong>: the model is trained with cross entropy loss, but to optimise the model to “focus” more on the “hard” labels, a gradient norm is calculated for each label (where “easy” labels have low gradients) and a weight for the loss calculation is assigned to each label based on the gradient density (histogram statistic). The idea is to decrease the weight of loss form labels with low gradient norms.</p></li>
</ul>
<p>Architecture:</p>
<ul class="simple">
<li><p><strong>Activation</strong> function: GeLU (Gaussian Error Linear Unit, non-linear function that maps negative input values to negative outputs and positive input values to positive outputs)</p></li>
<li><p>Initial <strong>tokenization and embeddings</strong> (WordPiece, a subword tokenization method used for the original BERT model)</p>
<ul>
<li><p>A nn.Embeddings layer combines word embeddings, positional embeddings and token type embeddings (n=2)</p></li>
</ul>
</li>
<li><p>n x the <strong>encoder block</strong> (12 in this configuration, same as original BERT model)</p>
<ul>
<li><p>Multi-head Scaled-dot product attention with Softmax to generate context layer</p></li>
<li><p>‘Intermediate’: linear layer and activation function</p></li>
<li><p>‘Output’: liner layer, layer normalisation, dropout</p></li>
</ul>
</li>
<li><p>The <strong>classification head</strong> for ATE (nn.Linear, Softmax)</p></li>
<li><p>n x the <strong>decoder block</strong> (2 in this configuration)</p></li>
<li><p>The <strong>classification head</strong> for ASC (nn.Linear, Softmax)</p></li>
</ul>
<p>In this collapsed cell are a few key building blocks in coded implementation:</p>
<div class="cell tag_hide-cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># higher-level contents of one encoder
class BertLayer(nn.Module):
    def __init__(self, config):
        super(BertLayer, self).__init__()
        self.attention = BertAttention(config)  # multi-head scaled-dot prodcut self-attention
        self.intermediate = BertIntermediate(config)  # feed forward linear and normalisation
        self.output = BertOutput(config)

    def forward(self, hidden_states, attention_mask):
        attention_output = self.attention(hidden_states, attention_mask)
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        return layer_output



# An encoder block (x12 in GRACE)
class BertEncoder(nn.Module):
    def __init__(self, config):
        super(BertEncoder, self).__init__()
        layer = BertLayer(config)
        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)]) # 12 

    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):
        all_encoder_layers = []
        for layer_module in self.layer:
            hidden_states = layer_module(hidden_states, attention_mask)
            if output_all_encoded_layers:
                all_encoder_layers.append(hidden_states)
        if not output_all_encoded_layers:
            all_encoder_layers.append(hidden_states)
        return all_encoder_layers



 # pooler layer for summarization based on [CLS] token
class BertPooler(nn.Module):
    def __init__(self, config):
        super(BertPooler, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We &quot;pool&quot; the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output



# model class using pretrained configurations
class BertModel(PreTrainedBertModel):
    &quot;&quot;&quot;BERT model (&quot;Bidirectional Embedding Representations from a Transformer&quot;).
    Params:
        config: a BertConfig class instance with the configuration to build a new model
    Inputs:
        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]
            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts
            `extract_features.py`, `run_classifier.py` and `run_squad.py`)
        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token
            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to
            a `sentence B` token (see BERT paper for more details).
        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices
            selected in [0, 1]. It&#39;s a mask to be used if the input sequence length is smaller than the max
            input sequence length in the current batch. It&#39;s the mask that we typically use for attention when
            a batch has varying length sentences.
        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.
    Outputs: Tuple of (encoded_layers, pooled_output)
        `encoded_layers`: controled by `output_all_encoded_layers` argument:
            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end
                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each
                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],
            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding
                to the last attention block of shape [batch_size, sequence_length, hidden_size],
        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a
            classifier pretrained on top of the hidden state associated to the first character of the
            input (`CLF`) to train on the Next-Sentence task (see BERT&#39;s paper).
    Example usage:
    ```python
    # Already been converted into WordPiece token ids
    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])
    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])
    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])
    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,
        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)
    model = modeling.BertModel(config=config)
    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)
    ```



# BertModel then again is used to compose a whole classification task workflow, e.g.:
class BertForSequenceClassification(PreTrainedBertModel):
    &quot;&quot;&quot;BERT model for classification.
    This module is composed of the BERT model with a linear layer on top of
    the pooled output.
    Params:
        `config`: a BertConfig class instance with the configuration to build a new model.
        `num_labels`: the number of classes for the classifier. Default = 2.
    Inputs:
        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]
            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts
            `extract_features.py`, `run_classifier.py` and `run_squad.py`)
        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token
            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to
            a `sentence B` token (see BERT paper for more details).
        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices
            selected in [0, 1]. It&#39;s a mask to be used if the input sequence length is smaller than the max
            input sequence length in the current batch. It&#39;s the mask that we typically use for attention when
            a batch has varying length sentences.
        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]
            with indices selected in [0, ..., num_labels].
    Outputs:
        if `labels` is not `None`:
            Outputs the CrossEntropy classification loss of the output with the labels.
        if `labels` is `None`:
            Outputs the classification logits of shape [batch_size, num_labels].
    Example usage:
    ```python
    # Already been converted into WordPiece token ids
    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])
    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])
    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])
    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,
        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)
    num_labels = 2
    model = BertForSequenceClassification(config, num_labels)
    logits = model(input_ids, token_type_ids, input_mask)


# complete asc ate workflow initialisation:
class BertForSequenceLabeling(PreTrainedBertModel):
    #
    def __init__(self, config, num_tp_labels, task_config):
        super(BertForSequenceLabeling, self).__init__(config)
        self.num_tp_labels = num_tp_labels
        self.task_config = task_config

        at_label_list = self.task_config[&quot;at_labels&quot;]
        self.at_label_map = {i: label for i, label in enumerate(at_label_list)}
        assert len(at_label_list) == 3, &quot;Hard code works when doing BIO strategy, &quot; \
                                        &quot;due to the middle step to generate span boundary.&quot;
        self.I_AP_INDEX = 2     # Note: This operation works when doing BIO strategy
        assert self.at_label_map[self.I_AP_INDEX] == &quot;I-AP&quot;, &quot;A hard code need the index below.&quot;

        self.num_encoder_labels = self.num_tp_labels[0]
        self.bert = BertModel(config)
        self.classifier = nn.Linear(config.hidden_size, self.num_encoder_labels)

        if self.task_config[&quot;use_ghl&quot;]:
            self.weighted_ce_loss_fct = WeightedCrossEntropy(ignore_index=-1)
        else:
            self.ce_loss_fct = CrossEntropyLoss(ignore_index=-1)

        ## Gradient balance &lt;---
        self.bins = 24
        self.momentum = 0.75
        self.edges = torch.arange(self.bins + 1).float() / self.bins
        self.edges[-1] += 1e-6
        self.acc_sum = torch.zeros(self.bins, dtype=torch.float)

        self.decoder_bins = 24
        self.decoder_momentum = 0.75
        self.decoder_edges = torch.arange(self.bins + 1).float() / self.bins
        self.decoder_edges[-1] += 1e-6
        self.decoder_acc_sum = torch.zeros(self.bins, dtype=torch.float)
        self.decoder_weight_gradient = None
        self.decoder_weight_gradient_labels = None
        ## ---&gt;

        self.use_vat = self.task_config[&quot;use_vat&quot;]
        if self.use_vat:
            self.alpha = 1.
            self.xi = 1e-6
            self.epsilon = 2.
            self.ip = 1

        self.num_decoder_labels = self.num_tp_labels[1]
        if config.hidden_size == 768:
            decoder_config, _ = PreTrainedDecoderBertModel.get_config(&quot;decoder-bert-base&quot;)
        else:
            raise ValueError(&quot;No implementation on such a decoder config.&quot;)

        self.decoder_shared_layer = self.task_config[&quot;decoder_shared_layer&quot;]
        decoder_config.decoder_vocab_size = self.num_encoder_labels
        decoder_config.num_decoder_layers = self.task_config[&quot;num_decoder_layer&quot;]
        bert_position_embeddings_weight = self.bert.embeddings.position_embeddings.weight

        # NOTE: DecoderBertModel is adapted from the Transformer decoder.
        # It is not a decoder used as generation task. It is used as labeling task here.
        self.decoder = DecoderBertModel(decoder_config, bert_position_embeddings_weight)
        self.decoder_classifier = nn.Linear(config.hidden_size, self.num_decoder_labels)

        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = ACT2FN[config.hidden_act] # nn.Tanh()

        self.apply(self.init_bert_weights)

    # Virtual Adversarial Training Implementation
    def vat_loss(self, input_ids, token_type_ids, attention_mask):
        # LDS should be calculated before the forward for cross entropy
        with torch.no_grad():
            _pred_logits, _, _ = self.get_encoder_logits(input_ids, token_type_ids, attention_mask)
            pred = F.softmax(_pred_logits, dim=2)

        # prepare random unit tensor
        batch_size_, seq_length_ = input_ids.size()
        hidden_size_ = self.bert.config.hidden_size
        d = torch.randn(batch_size_, seq_length_, hidden_size_, device=input_ids.device)

        with _disable_tracking_bn_stats(self):
            # calc adversarial direction
            for _ in range(self.ip):
                d.requires_grad_()
                xi_d = self.xi * _l2_normalize_foremd(_mask_by_length(d, attention_mask))
                xi_d.retain_grad()
                words_embeddings_ = self.bert.embeddings.word_embeddings(input_ids)
                pred_hat, _, _ = self.get_encoder_logits(words_embeddings_ + xi_d, token_type_ids, attention_mask,
                                                   bool_input_embedding=True)
                logp_hat_i = F.log_softmax(pred_hat, dim=2).view(-1, self.num_encoder_labels)
                pred_i = pred.view(-1, self.num_encoder_labels)
                adv_distance = F.kl_div(logp_hat_i, pred_i, reduction=&#39;batchmean&#39;)
                adv_distance.backward()
                d = xi_d.grad
                self.zero_grad()

            # calc LDS
            r_adv = _l2_normalize_foremd(d.detach()) * self.epsilon
            words_embeddings_ = self.bert.embeddings.word_embeddings(input_ids)

            pred_hat, _, _ = self.get_encoder_logits(words_embeddings_+r_adv, token_type_ids, attention_mask,
                                               bool_input_embedding=True)
            logp_hat_i = F.log_softmax(pred_hat, dim=2).view(-1, self.num_encoder_labels)
            pred_i = pred.view(-1, self.num_encoder_labels)
            lds = F.kl_div(logp_hat_i, pred_i, reduction=&#39;batchmean&#39;)
        return lds

    # Gradient Harmonized Loss Implementation
    def calculate_ce_gradient_weight(self, logits, labels, attention_mask, num_labels,
                         acc_sum, bins, momentum, edges, weight_gradient=None, weight_gradient_labels=None):
        device = logits.device
        batch_size, sequence_length = labels.size()
        # Here using crf_label_ids for CE labels have -1 value.
        labels_onehot = torch.zeros(batch_size, sequence_length, num_labels, dtype=torch.float, device=device)
        crf_label_ids = labels.clone()
        crf_label_ids[crf_label_ids &lt; 0] = 0.
        labels_onehot.scatter_(2, crf_label_ids.unsqueeze(2), 1)
        # gradient length
        gradient = torch.abs(F.softmax(logits.detach(), dim=-1) - labels_onehot)

        weights, acc_sum, weight_gradient, weight_gradient_labels \
            = self.statistic_weight(gradient, logits, labels, attention_mask, num_labels,
                                    acc_sum, bins, momentum, edges, weight_gradient, weight_gradient_labels)

        return weights, acc_sum, weight_gradient, weight_gradient_labels

    def statistic_weight(self, gradient, logits, labels, attention_mask, num_labels,
                         acc_sum, bins, momentum, edges,
                         weight_gradient=None, weight_gradient_labels=None):
        device = logits.device
        batch_size, sequence_length = labels.size()

        if weight_gradient is None:
            weight_gradient = torch.zeros(self.bins).to(device)
        if weight_gradient_labels is None:
            weight_gradient_labels = torch.zeros(self.bins, num_labels).to(device)

        edges = self.edges.to(device)
        momentum = self.momentum
        weights = torch.ones_like(logits)

        valid_instance = attention_mask.unsqueeze(-1).expand(batch_size, sequence_length, num_labels)
        valid_instance = valid_instance &gt; 0
        total_valid = max(valid_instance.float().sum().item(), 1.0)
        n = 0  # n valid bins
        for i in range(self.bins):
            inds = (gradient &gt;= edges[i]) &amp; (gradient &lt; edges[i + 1]) &amp; valid_instance

            num_in_bin_label = inds.sum(0).sum(0).to(dtype=weight_gradient_labels.dtype)
            weight_gradient_labels[i, :] = weight_gradient_labels[i, :] + num_in_bin_label

            num_in_bin = inds.sum().item()

            weight_gradient[i] = weight_gradient[i] + num_in_bin

            if num_in_bin &gt; 0:
                if momentum &gt; 0:
                    index_tensor = torch.tensor(i)
                    val_ = torch.gather(acc_sum, dim=0, index=index_tensor)
                    momentum_bins = momentum * float(val_.item()) + (1 - momentum) * num_in_bin
                    weights[inds] = total_valid / momentum_bins
                    acc_sum.scatter_(0, index_tensor, momentum_bins)
                else:
                    weights[inds] = total_valid / num_in_bin
                n += 1

        return weights, acc_sum, weight_gradient, weight_gradient_labels
</pre></div>
</div>
</div>
</div>
<img src="https://raw.githubusercontent.com/nlp-with-transformers/notebooks/e3850199388f4983cc9799135977f0a6b06d5a79//images/chapter03_transformer-encoder-decoder.png"><p>In replicating the training described in the paper using a twitter training dataset (<code class="docutils literal notranslate"><span class="pre">twt1</span></code>), an F1 score of 0.7514 was achieved for aspect term extraction. However, for the aspect sentiment classification, an F1 score of only 0.5694 was achieved.</p>
<p>Future work may investigate whether higher results may be achieved for the GRACE ASC sub-task to fully leverage an E2E model for twitter ABSA.</p>
</section>
</section>
<section id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">#</a></h2>
<section id="grace-model-setup-loading-from-last-training-step-and-epoch">
<h3>GRACE Model Setup (Loading from last training Step and Epoch)<a class="headerlink" href="#grace-model-setup-loading-from-last-training-step-and-epoch" title="Permalink to this headline">#</a></h3>
<p>log messages from last training step and epoch: <br></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
<span class="n">Model</span> <span class="n">saved</span> <span class="n">to</span> <span class="n">out_twt1_ateacs</span><span class="o">/</span><span class="n">pytorch_model</span><span class="o">.</span><span class="n">bin</span><span class="mf">.9</span>

<span class="n">AT</span> <span class="n">p</span><span class="p">:</span><span class="mf">0.7365</span> 	<span class="n">r</span><span class="p">:</span><span class="mf">0.7670</span>	<span class="n">f1</span><span class="p">:</span><span class="mf">0.7514</span> 

<span class="n">AS</span> <span class="n">p</span><span class="p">:</span><span class="mf">0.5581</span> 	<span class="n">r</span><span class="p">:</span><span class="mf">0.5811</span>	<span class="n">f1</span><span class="p">:</span><span class="mf">0.5694</span> 


</pre></div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># args set as per instructions by authors</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">(</span>

    <span class="c1">## Required parameters</span>
    <span class="n">data_dir</span><span class="o">=</span><span class="s1">&#39;../GRACE/data/&#39;</span><span class="p">,</span> 
    <span class="n">bert_model</span><span class="o">=</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span>
    <span class="n">init_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">task_name</span><span class="o">=</span><span class="s2">&quot;ate_asc&quot;</span><span class="p">,</span>
    <span class="n">data_name</span><span class="o">=</span><span class="s2">&quot;twt1&quot;</span><span class="p">,</span>
    <span class="n">train_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">valid_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">test_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;out_testing/&#39;</span><span class="p">,</span> 
    
    <span class="c1">## Other parameters</span>
    <span class="n">cache_dir</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">do_train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">do_eval</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">do_lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">train_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> 
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">3e-06</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
    <span class="n">warmup_proportion</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
    <span class="n">num_thread_reader</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
    <span class="n">no_cuda</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">local_rank</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> 
    <span class="n">fp16</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">loss_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">verbose_logging</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">server_ip</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span>
    <span class="n">server_port</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> 
    <span class="n">use_ghl</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">use_vat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">use_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">num_decoder_layer</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
    <span class="n">decoder_shared_layer</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AT Labels are: [O, B-AP, I-AP]
AS Labels are: [O, NEGATIVE, POSITIVE, NEUTRAL]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">model_file</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">num_tp_labels</span><span class="p">,</span> <span class="n">task_config</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">model_file</span> <span class="o">=</span> <span class="n">model_file</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">model_file</span><span class="p">):</span>
        <span class="n">model_state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_file</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model loaded from </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">model_file</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceLabeling</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">bert_model</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="n">PYTORCH_PRETRAINED_BERT_CACHE</span> <span class="o">/</span> <span class="s1">&#39;distributed_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">),</span>
                                                        <span class="n">state_dict</span><span class="o">=</span><span class="n">model_state_dict</span><span class="p">,</span> <span class="n">num_tp_labels</span><span class="o">=</span><span class="n">num_tp_labels</span><span class="p">,</span>
                                                        <span class="n">task_config</span><span class="o">=</span><span class="n">task_config</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># set model file to the last saved model after training epochs completed</span>

<span class="c1"># CODE CHANGE NOTE: In this implementation (using a docker container for jupyter lab) the download of bert-base-uncased.tar.gz </span>
<span class="c1">#into cache terminates before the whole file is successfully loaded. </span>
<span class="c1"># Therefore an adapted ate_asc_modeling_local_bert_file.py is imported here which loads the model from a folder in the repo (&#39;bert-base-uncased/bert-base-uncased.tar.gz&#39;)</span>

<span class="n">model_file</span> <span class="o">=</span> <span class="s1">&#39;../GRACE/out_twt1_ateacs/pytorch_model.bin.9&#39;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">model_file</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">num_tp_labels</span><span class="p">,</span> <span class="n">task_config</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The model summary:</p>
<div class="cell tag_hide-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BertForSequenceLabeling(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): BertLayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=3, bias=True)
  (weighted_ce_loss_fct): WeightedCrossEntropy()
  (decoder): DecoderBertModel(
    (embeddings): BertDecoderEmbeddings(
      (decoder_word_embeddings): Embedding(3, 768)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): DecoderBertLayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (decoder): BertDecoder(
      (layer): ModuleList(
        (0): DecoderLayer(
          (slf_attn): DecoderAttention(
            (att): MultiHeadAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): DecoderBertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (enc_attn): DecoderAttention(
            (att): MultiHeadAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): DecoderBertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): DecoderBertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): DecoderLayer(
          (slf_attn): DecoderAttention(
            (att): MultiHeadAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): DecoderBertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (enc_attn): DecoderAttention(
            (att): MultiHeadAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): DecoderBertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): DecoderBertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (decoder_classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (dense): Linear(in_features=768, out_features=768, bias=True)
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="apply-model-on-twemlab-goldstandard-data">
<h3>Apply Model on Twemlab Goldstandard Data<a class="headerlink" href="#apply-model-on-twemlab-goldstandard-data" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load TwEmLab Goldstandard</span>
<span class="n">tree1</span> <span class="o">=</span> <span class="n">ET</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s1">&#39;../Data/twemlab_goldstandards_original/birmingham_labels.xml&#39;</span><span class="p">)</span>
<span class="n">root1</span> <span class="o">=</span> <span class="n">tree1</span><span class="o">.</span><span class="n">getroot</span><span class="p">()</span>

<span class="c1"># create dataframe from xml file</span>
<span class="n">data1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">root1</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;Tweet&#39;</span><span class="p">):</span>
    <span class="nb">id</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;ID&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;Label&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span>
    <span class="n">data1</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">id</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>
 <span class="c1"># df1.head()</span>
    
<span class="c1"># Load TwEmLab Boston Tweets</span>
<span class="n">tree2</span> <span class="o">=</span> <span class="n">ET</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s1">&#39;../Data/twemlab_goldstandards_original/birmingham_tweets.xml&#39;</span><span class="p">)</span>
<span class="n">root2</span> <span class="o">=</span> <span class="n">tree2</span><span class="o">.</span><span class="n">getroot</span><span class="p">()</span>

<span class="c1"># create dataframe from xml file</span>
<span class="n">data2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">root2</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;Tweet&#39;</span><span class="p">):</span>
    <span class="nb">id</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;ID&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span>
    <span class="n">goldstandard</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">attrib</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;goldstandard&quot;</span><span class="p">)</span>
    <span class="n">data2</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">id</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">goldstandard</span><span class="p">))</span>

<span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;goldstandard&#39;</span><span class="p">])</span>
<span class="c1"># df2.head()</span>

 <span class="c1"># merge the two separate dataframes based on id columns</span>
<span class="n">merge</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s1">&#39;id&#39;</span><span class="p">)</span>

<span class="c1"># keep only the tweets that are part of the goldstandard</span>
<span class="n">twemlab</span> <span class="o">=</span> <span class="n">merge</span><span class="p">[</span><span class="n">merge</span><span class="p">[</span><span class="s1">&#39;goldstandard&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;yes&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of tweets in goldstandard: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">twemlab</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">sentimemt_label_three</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># assign sentiment label (0, 1) based on emotion</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">twemlab</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;beauty&#39;</span> <span class="ow">or</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;happiness&#39;</span><span class="p">:</span>
        <span class="n">sentimemt_label_three</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
        <span class="n">sentimemt_label_three</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span> 
        <span class="n">sentimemt_label_three</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
<span class="n">twemlab</span><span class="p">[</span><span class="s1">&#39;sentiment_label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sentimemt_label_three</span>

<span class="c1"># check dataset</span>
<span class="n">twemlab</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of tweets in goldstandard: 994
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>label</th>
      <th>text</th>
      <th>goldstandard</th>
      <th>sentiment_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>200000000000000001</td>
      <td>beauty</td>
      <td>who says summer is over; beautiful run in Edin...</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>200000000000000002</td>
      <td>none</td>
      <td>Eid prayer in small heath park 7:30am sharp to...</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>200000000000000003</td>
      <td>none</td>
      <td>did the last one at Summerfield Park</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>200000000000000004</td>
      <td>none</td>
      <td>that was Summerfield Park</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>200000000000000005</td>
      <td>none</td>
      <td>FREE led cycle ride from Edgbaston Reservoir, ...</td>
      <td>yes</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># empty lists for outputs</span>
<span class="n">pred_aspect_terms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">pred_aspect_sentiments</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># eval_dataloader contains the pre-processed, tokenized inputs (input has been reformatted for GRACE beforehand)</span>
<span class="k">for</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">at_label_ids</span><span class="p">,</span> <span class="n">as_label_ids</span><span class="p">,</span> <span class="n">label_mask</span><span class="p">,</span> <span class="n">label_mask_X</span> <span class="ow">in</span> <span class="n">eval_dataloader</span><span class="p">:</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">input_mask</span> <span class="o">=</span> <span class="n">input_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">segment_ids</span> <span class="o">=</span> <span class="n">segment_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">at_label_ids</span> <span class="o">=</span> <span class="n">at_label_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">as_label_ids</span> <span class="o">=</span> <span class="n">as_label_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">label_mask</span> <span class="o">=</span> <span class="n">label_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">label_mask_X</span> <span class="o">=</span> <span class="n">label_mask_X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># get predictions with argmax log-softmax probabilities</span>
    <span class="c1"># predicted aspect term ids from logits (encoder)</span>
    <span class="c1"># predicted asepct sentiment ids from decoder logits</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># logits, decoder_logits = model(input_ids, segment_ids, input_mask)</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">sequence_output</span><span class="p">,</span> <span class="n">encoder_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_encoder_logits</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">)</span>
        <span class="n">pred_dec_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">decoder_logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_decoder_logits</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">label_mask_X</span><span class="p">,</span> <span class="n">pred_dec_ids</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">decoder_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">decoder_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">decoder_logits</span> <span class="o">=</span> <span class="n">decoder_logits</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
    <span class="n">at_label_ids</span> <span class="o">=</span> <span class="n">at_label_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">as_label_ids</span> <span class="o">=</span> <span class="n">as_label_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">label_mask</span> <span class="o">=</span> <span class="n">label_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">mask_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">label_mask</span><span class="p">):</span>
        <span class="c1">#temp_11 = []</span>
        <span class="n">temp_12</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1">#temp_21 = []</span>
        <span class="n">temp_22</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mask_i</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">l</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="c1"># no at_label_ids or as_label_ids because there is no ground truth data in this dataset</span>
                <span class="c1">#temp_11.append(at_label_map[at_label_ids[i][j]])</span>
                <span class="n">temp_12</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">at_label_map</span><span class="p">[</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]])</span>
                <span class="c1">#temp_21.append(as_label_map[as_label_ids[i][j]])</span>
                <span class="n">temp_22</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">as_label_map</span><span class="p">[</span><span class="n">decoder_logits</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]])</span>
                
        <span class="n">pred_aspect_terms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_12</span><span class="p">)</span>
        <span class="n">pred_aspect_sentiments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_22</span><span class="p">)</span>

<span class="c1"># add new aspect term labels and aspect sentiment labels as columns to twemlab dataframe</span>
<span class="n">twemlab</span><span class="p">[</span><span class="s1">&#39;aspect_term_preds&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_aspect_terms</span>
<span class="n">twemlab</span><span class="p">[</span><span class="s1">&#39;aspect_senti_preds&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_aspect_sentiments</span>
</pre></div>
</div>
</div>
</div>
<p>Extract Aspect Terms and Sentiment Aspects</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># extract aspect term and sentiment and store in twemlab dataframe</span>
<span class="n">aspect_terms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">aspect_sentiments</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># for every row (tweet)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">twemlab</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    
    <span class="n">row_aspect_terms</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">row_aspect_sentiments</span> <span class="o">=</span> <span class="p">[]</span> 

    <span class="c1"># get text length</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">count_words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    
    <span class="c1"># get token length (may differ from text length)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_senti_preds&#39;</span><span class="p">]</span>
    <span class="n">token_counts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    
    <span class="c1"># for every word in tweet --&gt; check if it&#39;s an aspect term and save</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count_words</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_term_preds&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;B-AP&#39;</span><span class="p">:</span>
            <span class="n">term</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                   
            <span class="c1"># for remaining words</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">count_words</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_term_preds&#39;</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;I-AP&#39;</span><span class="p">:</span>
                    <span class="n">term</span> <span class="o">=</span> <span class="n">term</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">words</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            
            <span class="n">row_aspect_terms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">term</span><span class="p">)</span>
    
    <span class="n">aspect_terms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row_aspect_terms</span><span class="p">)</span>
    
    <span class="c1"># for every token --&gt; extract sentiment</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">token_counts</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_term_preds&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;B-AP&#39;</span><span class="p">:</span>
            <span class="n">sent</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            
            <span class="c1"># for remaining tokens</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">token_counts</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_term_preds&#39;</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;I-AP&#39;</span><span class="p">:</span>
                    <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">tokens</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            
            <span class="n">row_aspect_sentiments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
    
    <span class="n">aspect_sentiments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row_aspect_sentiments</span><span class="p">)</span>
            
<span class="n">twemlab</span><span class="p">[</span><span class="s1">&#39;aspect_terms&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">aspect_terms</span>
<span class="n">twemlab</span><span class="p">[</span><span class="s1">&#39;aspect_sentiments&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">aspect_sentiments</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_e4f33 th {
  text-align: left;
}
#T_e4f33_row0_col0, #T_e4f33_row0_col1, #T_e4f33_row0_col2, #T_e4f33_row0_col3, #T_e4f33_row0_col4, #T_e4f33_row1_col0, #T_e4f33_row1_col1, #T_e4f33_row1_col2, #T_e4f33_row1_col3, #T_e4f33_row1_col4, #T_e4f33_row2_col0, #T_e4f33_row2_col1, #T_e4f33_row2_col2, #T_e4f33_row2_col3, #T_e4f33_row2_col4, #T_e4f33_row3_col0, #T_e4f33_row3_col1, #T_e4f33_row3_col2, #T_e4f33_row3_col3, #T_e4f33_row3_col4, #T_e4f33_row4_col0, #T_e4f33_row4_col1, #T_e4f33_row4_col2, #T_e4f33_row4_col3, #T_e4f33_row4_col4 {
  text-align: left;
}
</style>
<table id="T_e4f33">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_e4f33_level0_col0" class="col_heading level0 col0" >Text</th>
      <th id="T_e4f33_level0_col1" class="col_heading level0 col1" >Label</th>
      <th id="T_e4f33_level0_col2" class="col_heading level0 col2" >sentiment_label</th>
      <th id="T_e4f33_level0_col3" class="col_heading level0 col3" >Aspect Terms</th>
      <th id="T_e4f33_level0_col4" class="col_heading level0 col4" >Aspect Sentiments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_e4f33_level0_row0" class="row_heading level0 row0" >904</th>
      <td id="T_e4f33_row0_col0" class="data row0 col0" >Christmas Fayre Sat 12 Dec 1.30 to 4.30 Hole Farm Trekking Centre Woodgate Valley Country Park Grotto Pony Rides Stalls Raffle Refreshments</td>
      <td id="T_e4f33_row0_col1" class="data row0 col1" >none</td>
      <td id="T_e4f33_row0_col2" class="data row0 col2" >0</td>
      <td id="T_e4f33_row0_col3" class="data row0 col3" >['Woodgate Valley Country Park']</td>
      <td id="T_e4f33_row0_col4" class="data row0 col4" >['NEUTRAL NEUTRAL NEUTRAL NEUTRAL']</td>
    </tr>
    <tr>
      <th id="T_e4f33_level0_row1" class="row_heading level0 row1" >644</th>
      <td id="T_e4f33_row1_col0" class="data row1 col0" >Sunshine love in moseley park yesterday. #babysmiles </td>
      <td id="T_e4f33_row1_col1" class="data row1 col1" >happiness</td>
      <td id="T_e4f33_row1_col2" class="data row1 col2" >1</td>
      <td id="T_e4f33_row1_col3" class="data row1 col3" >['moseley']</td>
      <td id="T_e4f33_row1_col4" class="data row1 col4" >['POSITIVE']</td>
    </tr>
    <tr>
      <th id="T_e4f33_level0_row2" class="row_heading level0 row2" >841</th>
      <td id="T_e4f33_row2_col0" class="data row2 col0" >Running on treadmill is soul destroying, give me Sutton Park in sun, wind, rain or snow any day
 
#NoBrainer</td>
      <td id="T_e4f33_row2_col1" class="data row2 col1" >happiness</td>
      <td id="T_e4f33_row2_col2" class="data row2 col2" >1</td>
      <td id="T_e4f33_row2_col3" class="data row2 col3" >[]</td>
      <td id="T_e4f33_row2_col4" class="data row2 col4" >[]</td>
    </tr>
    <tr>
      <th id="T_e4f33_level0_row3" class="row_heading level0 row3" >132</th>
      <td id="T_e4f33_row3_col0" class="data row3 col0" >Game postponed due to waterlogged pitch in Cardiff :( Boys putting a shift in at selly park as countdown to the uni game starts</td>
      <td id="T_e4f33_row3_col1" class="data row3 col1" >sadness</td>
      <td id="T_e4f33_row3_col2" class="data row3 col2" >-1</td>
      <td id="T_e4f33_row3_col3" class="data row3 col3" >['uni']</td>
      <td id="T_e4f33_row3_col4" class="data row3 col4" >['NEUTRAL']</td>
    </tr>
    <tr>
      <th id="T_e4f33_level0_row4" class="row_heading level0 row4" >769</th>
      <td id="T_e4f33_row4_col0" class="data row4 col0" >...heading down to Moseley Bog for the official unofficial festival inabit. coming?   </td>
      <td id="T_e4f33_row4_col1" class="data row4 col1" >none</td>
      <td id="T_e4f33_row4_col2" class="data row4 col2" >0</td>
      <td id="T_e4f33_row4_col3" class="data row4 col3" >['Moseley Bog']</td>
      <td id="T_e4f33_row4_col4" class="data row4 col4" >['NEUTRAL NEUTRAL']</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>An Example of how the GRACE results look

Text:	Next Sat  Our Big Gig Festival in Summerfield Park  

Asp.Terms:	[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-AP&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;]
Asp.Sents:	[&#39;POSITIVE&#39;, &#39;POSITIVE&#39;, &#39;POSITIVE&#39;, &#39;POSITIVE&#39;, &#39;POSITIVE&#39;, &#39;POSITIVE&#39;, &#39;POSITIVE&#39;, &#39;POSITIVE&#39;, &#39;POSITIVE&#39;]

ATE:	[&#39;Gig&#39;]
ASC:	[&#39;POSITIVE&#39;]

Goldstandard: 0
</pre></div>
</div>
</div>
</div>
<p>To get an indication of how well the model is classifying the aspect terms and their sentiments, a make-shift comparison to the goldstandard labels can me made. Note that the goldstandard annotations differ from the GRACE model output:</p>
<ul class="simple">
<li><p>The goldstandard was labelled with emotions, whereas GRACE now identifies sentments (pos, neu, neg)</p></li>
<li><p>The goldstandard was labelled per document, i.e. the entire tweet, whereas GRACE breaks down the document into the token level and can identify several aspects within one tweet, each potentially with differnt sentiments</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compare the aspect sentiments with the twemlab sentiments</span>

<span class="n">match</span> <span class="o">=</span> <span class="mi">0</span>          <span class="c1"># sentiment tokens uniformly match goldstandard sentiment (incl. no-list on sentiment type &quot;none&quot;)</span>
<span class="n">nomatch</span> <span class="o">=</span> <span class="mi">0</span>        <span class="c1"># sentiment tokens are uniformly different to goldstandard sentiment</span>
<span class="n">not_uniform</span> <span class="o">=</span> <span class="mi">0</span>    <span class="c1"># sentiment tokens are not uniform</span>
<span class="n">n_a</span> <span class="o">=</span> <span class="mi">0</span>            <span class="c1"># empty sentiment tokens list (that does not match with sentiment label &#39;none&#39;)</span>
<span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>        <span class="c1"># overall rows</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">twemlab</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="c1"># if there is more than 1 list of aspect sentiment tokens</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_sentiments&#39;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        
        <span class="c1"># join them together into one list</span>
        <span class="n">all_tokens</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_sentiments&#39;</span><span class="p">])</span>
        <span class="n">all_tokens</span> <span class="o">=</span> <span class="n">all_tokens</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">first_token</span> <span class="o">=</span> <span class="n">all_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># check if tokens are the same, e.g. [&#39;NEUTRAL NEUTRAL&#39;, &#39;NEUTRAL NEUTRAL&#39;]</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">token</span> <span class="o">==</span> <span class="n">first_token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">all_tokens</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">first_token</span> <span class="o">==</span> <span class="s1">&#39;POSITIVE&#39;</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="n">first_token</span> <span class="o">==</span> <span class="s1">&#39;NEUTRAL&#39;</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="n">first_token</span> <span class="o">==</span> <span class="s1">&#39;NEGATIVE&#39;</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span> <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span> <span class="n">nomatch</span> <span class="o">+=</span> <span class="mi">1</span> 
        <span class="k">else</span><span class="p">:</span> <span class="n">not_uniform</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># if there is just 1 list of aspect sentiment tokens</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_sentiments&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_sentiments&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">first_token</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># if the list has more than one token</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># check if tokens are the same  e.g. [&#39;POSITIVE POSITIVE POSITIVE&#39;]</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">token</span> <span class="o">==</span> <span class="n">first_token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">):</span>
                <span class="c1">#print(f&quot;row asp sents: {row[&#39;aspect_sentiments&#39;]}&quot;)</span>
                <span class="k">if</span> <span class="n">first_token</span> <span class="o">==</span> <span class="s1">&#39;POSITIVE&#39;</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">elif</span> <span class="n">first_token</span> <span class="o">==</span> <span class="s1">&#39;NEUTRAL&#39;</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">elif</span> <span class="n">first_token</span> <span class="o">==</span> <span class="s1">&#39;NEGATIVE&#39;</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span> <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span> <span class="n">nomatch</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span> <span class="n">not_uniform</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="c1"># if the list has only one entry</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_sentiments&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;POSITIVE&#39;</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_sentiments&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;NEUTRAL&#39;</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;aspect_sentiments&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;NEGATIVE&#39;</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span> <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span> <span class="n">nomatch</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;issue&quot;</span><span class="p">)</span>
    
    <span class="c1"># empty list</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span> <span class="n">n_a</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Comparing the aspect sentiment classifcation with the overall sentiment in the twemlab goldstandard:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Matches:      </span><span class="si">{</span><span class="n">match</span><span class="si">}</span><span class="s2">  (</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">match</span><span class="o">/</span><span class="n">counter</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">%)</span><span class="se">\n</span><span class="s2">No Matches:   </span><span class="si">{</span><span class="n">nomatch</span><span class="si">}</span><span class="s2">  (</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">nomatch</span><span class="o">/</span><span class="n">counter</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">%)</span><span class="se">\n</span><span class="s2">Not Uniform:   </span><span class="si">{</span><span class="n">not_uniform</span><span class="si">}</span><span class="s2">   (</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">not_uniform</span><span class="o">/</span><span class="n">counter</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">%)</span><span class="se">\n</span><span class="s2">N/A:          </span><span class="si">{</span><span class="n">n_a</span><span class="si">}</span><span class="s2">  (</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">n_a</span><span class="o">/</span><span class="n">counter</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Comparing the aspect sentiment classifcation with the overall sentiment in the twemlab goldstandard:

Matches:      613  (61.7%)
No Matches:   194  (19.5%)
Not Uniform:   18   (1.8%)
N/A:          169  (17.0%)
</pre></div>
</div>
</div>
</div>
</section>
<section id="apply-model-on-aifer-twitter-dataset">
<h3>Apply Model on AIFER Twitter Dataset<a class="headerlink" href="#apply-model-on-aifer-twitter-dataset" title="Permalink to this headline">#</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>text</th>
      <th>geom</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>10749</th>
      <td>2021-07-31 17:04:21</td>
      <td>@MDegen55 @debbslovesnate I wish you a Good ni...</td>
      <td>POINT (7.265915499999999 50.45803549999999)</td>
    </tr>
    <tr>
      <th>7070</th>
      <td>2021-07-21 17:19:01</td>
      <td>@MDegen55 All Stana Katic Fans of the World. H...</td>
      <td>POINT (7.265915499999999 50.45803549999999)</td>
    </tr>
    <tr>
      <th>8624</th>
      <td>2021-07-25 08:56:22</td>
      <td>@MrsUnloveable Falle... 😁😁😁</td>
      <td>POINT (7.589509999999999 50.34630250000001)</td>
    </tr>
    <tr>
      <th>1269</th>
      <td>2021-07-05 16:09:07</td>
      <td>@MDegen55 Good night &amp; nice Tuesday @LawrenCal...</td>
      <td>POINT (7.265915499999999 50.45803549999999)</td>
    </tr>
    <tr>
      <th>5313</th>
      <td>2021-07-18 07:54:22</td>
      <td>@BuddyCoco64 @Jenny_S3005 @ASaramantinou @inax...</td>
      <td>POINT (7.265915499999999 50.45803549999999)</td>
    </tr>
    <tr>
      <th>1515</th>
      <td>2021-07-06 11:02:33</td>
      <td>@nlopes952 @pocs80 @AnetteRuff1 @piaroos1 @moa...</td>
      <td>POINT (7.265915499999999 50.45803549999999)</td>
    </tr>
    <tr>
      <th>4483</th>
      <td>2021-07-16 07:00:53</td>
      <td>@MDegen55 Hello AnnaBionda. Have a magical Fri...</td>
      <td>POINT (7.265915499999999 50.45803549999999)</td>
    </tr>
    <tr>
      <th>5306</th>
      <td>2021-07-18 07:48:39</td>
      <td>@pocs80 @Jenny_S3005 @MDegen55 Good morning Ju...</td>
      <td>POINT (7.265915499999999 50.45803549999999)</td>
    </tr>
    <tr>
      <th>4494</th>
      <td>2021-07-16 07:16:28</td>
      <td>@MDegen55 GOOD MORNING GIANNIS. HAVE A BEAUTIF...</td>
      <td>POINT (7.265915499999999 50.45803549999999)</td>
    </tr>
    <tr>
      <th>3865</th>
      <td>2021-07-13 21:23:12</td>
      <td>@MDegen55 @KatanaHugo Good evening and good Ni...</td>
      <td>POINT (7.265915499999999 50.45803549999999)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>A sample of the added columns:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_92ccb th {
  text-align: left;
}
#T_92ccb_row0_col0, #T_92ccb_row0_col1, #T_92ccb_row0_col2, #T_92ccb_row1_col0, #T_92ccb_row1_col1, #T_92ccb_row1_col2, #T_92ccb_row2_col0, #T_92ccb_row2_col1, #T_92ccb_row2_col2, #T_92ccb_row3_col0, #T_92ccb_row3_col1, #T_92ccb_row3_col2, #T_92ccb_row4_col0, #T_92ccb_row4_col1, #T_92ccb_row4_col2 {
  text-align: left;
}
</style>
<table id="T_92ccb">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_92ccb_level0_col0" class="col_heading level0 col0" >Text</th>
      <th id="T_92ccb_level0_col1" class="col_heading level0 col1" >Aspect Terms</th>
      <th id="T_92ccb_level0_col2" class="col_heading level0 col2" >Aspect Sentiments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_92ccb_level0_row0" class="row_heading level0 row0" >7640</th>
      <td id="T_92ccb_row0_col0" class="data row0 col0" >Please support my GoFundMe campaign: https://t.co/D2UUtzdcHF #GoFundMe https://t.co/sP7e9hTUBd</td>
      <td id="T_92ccb_row0_col1" class="data row0 col1" >['GoFundMe']</td>
      <td id="T_92ccb_row0_col2" class="data row0 col2" >['NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL']</td>
    </tr>
    <tr>
      <th id="T_92ccb_level0_row1" class="row_heading level0 row1" >4706</th>
      <td id="T_92ccb_row1_col0" class="data row1 col0" >@bancroft_lotty @SAMARASGIANNIS1 @iges2u @Jenny_S3005 @ASaramantinou @Heike622 @Annie2727 @inaxx68 @BuddyCoco64 @DeniseJoy71 @shannon_hausen @merryannie @KateBeckett8700 @KBRCAlways @captain_beckett @BitzIngeborg @Dalgaard2Mette @nadinesyd @pasburysmith @MDegen55 I wish you a Good night Lotty, sleep well and have a beautiful weekend and happy Saturday. 💚💙 https://t.co/D6IBgz32na</td>
      <td id="T_92ccb_row1_col1" class="data row1 col1" >['Lotty,']</td>
      <td id="T_92ccb_row1_col2" class="data row1 col2" >['NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'POSITIVE', 'POSITIVE', 'NEUTRAL', 'POSITIVE', 'POSITIVE', 'NEUTRAL', 'POSITIVE', 'POSITIVE', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'NEUTRAL', 'NEUTRAL']</td>
    </tr>
    <tr>
      <th id="T_92ccb_level0_row2" class="row_heading level0 row2" >9759</th>
      <td id="T_92ccb_row2_col0" class="data row2 col0" >Is Fagmeat a complete dickhead</td>
      <td id="T_92ccb_row2_col1" class="data row2 col1" >['Fagmeat']</td>
      <td id="T_92ccb_row2_col2" class="data row2 col2" >['NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE']</td>
    </tr>
    <tr>
      <th id="T_92ccb_level0_row3" class="row_heading level0 row3" >8240</th>
      <td id="T_92ccb_row3_col0" class="data row3 col0" >@PaulCXBI No, all fixed up and ready to roll by 7!</td>
      <td id="T_92ccb_row3_col1" class="data row3 col1" >['@PaulCXBI']</td>
      <td id="T_92ccb_row3_col2" class="data row3 col2" >['NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL']</td>
    </tr>
    <tr>
      <th id="T_92ccb_level0_row4" class="row_heading level0 row4" >9938</th>
      <td id="T_92ccb_row4_col0" class="data row4 col0" >@billgates Joe briden https://t.co/IIwZt8PmvM</td>
      <td id="T_92ccb_row4_col1" class="data row4 col1" >['@billgates', 'Joe briden']</td>
      <td id="T_92ccb_row4_col2" class="data row4 col2" >['NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL']</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Deep_Learning.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Document Level Sentiment Analysis with Deep Learning Models</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="GRACE_trained_for_ABEA_on_twitter_data.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Aspect-Based Emotion Analysis on Twitter Data</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book Community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>