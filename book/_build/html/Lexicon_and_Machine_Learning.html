
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Document-Level Sentiment Analysis with Lexicon and Machine Learning Methods &#8212; Sentiment Analysis</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Document Level Sentiment Analysis with Deep Learning Models" href="Deep_Learning.html" />
    <link rel="prev" title="Introduction to Sentiment Analysis" href="Introduction_to_SA.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo1.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Sentiment Analysis</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Overview
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction_to_SA.html">
   Introduction to Sentiment Analysis
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Document-Level Sentiment Analysis with Lexicon and Machine Learning Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Deep_Learning.html">
   Document Level Sentiment Analysis with Deep Learning Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="GRACE_trained_for_ABSA_on_twitter_data.html">
   Aspect-Based Sentiment Analysis on Twitter Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="GRACE_trained_for_ABEA_on_twitter_data.html">
   Aspect-Based Emotion Analysis on Twitter Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Twemlab_Aspect_Term_Annotation.html">
   Annotating Twemlab Goldstandard Files to Include Aspect Term Labels
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Datasets.html">
   Datasets for Sentiment Analysis and Aspect-Based Sentiment Analysis
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/Lexicon_and_Machine_Learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FLexicon_and_Machine_Learning.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/Lexicon_and_Machine_Learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rule-based-lexicon-approaches">
   Rule-Based/ Lexicon Approaches
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-approach-naive-bayes">
   Machine Learning Approach: Naive Bayes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#naive-bayes-functions">
     Naive Bayes Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-approach-support-vector-machine">
   Machine Learning Approach: Support Vector Machine
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Document-Level Sentiment Analysis with Lexicon and Machine Learning Methods</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rule-based-lexicon-approaches">
   Rule-Based/ Lexicon Approaches
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-approach-naive-bayes">
   Machine Learning Approach: Naive Bayes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#naive-bayes-functions">
     Naive Bayes Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-approach-support-vector-machine">
   Machine Learning Approach: Support Vector Machine
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="document-level-sentiment-analysis-with-lexicon-and-machine-learning-methods">
<h1>Document-Level Sentiment Analysis with Lexicon and Machine Learning Methods<a class="headerlink" href="#document-level-sentiment-analysis-with-lexicon-and-machine-learning-methods" title="Permalink to this headline">#</a></h1>
<p>Demo Dataset from the AIFER Project</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AIFER example dataset size: 11177
</pre></div>
</div>
<div class="output text_html"><style type="text/css">
#T_dcd19 th {
  text-align: left;
}
#T_dcd19_row0_col0, #T_dcd19_row0_col1, #T_dcd19_row0_col2, #T_dcd19_row1_col0, #T_dcd19_row1_col1, #T_dcd19_row1_col2, #T_dcd19_row2_col0, #T_dcd19_row2_col1, #T_dcd19_row2_col2 {
  text-align: left;
}
</style>
<table id="T_dcd19">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_dcd19_level0_col0" class="col_heading level0 col0" >Date</th>
      <th id="T_dcd19_level0_col1" class="col_heading level0 col1" >Text</th>
      <th id="T_dcd19_level0_col2" class="col_heading level0 col2" >Language</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_dcd19_level0_row0" class="row_heading level0 row0" >3204</th>
      <td id="T_dcd19_row0_col0" class="data row0 col0" >2021-07-11 13:38:35</td>
      <td id="T_dcd19_row0_col1" class="data row0 col1" >was ein Tipp: bist du arm dran und kannst es dir nicht leisten iwo besser zu leben na dann hast du zum gl√ºck neben einer viel befahrenenstra√üe weniger Miete +  hohes Gesundheitsrisikoü§©ü§©üòçüòç  
... CDU einfach eine schande ü§Æ</td>
      <td id="T_dcd19_row0_col2" class="data row0 col2" >de</td>
    </tr>
    <tr>
      <th id="T_dcd19_level0_row1" class="row_heading level0 row1" >10490</th>
      <td id="T_dcd19_row1_col0" class="data row1 col0" >2021-07-30 20:58:41</td>
      <td id="T_dcd19_row1_col1" class="data row1 col1" >@castlekitten47 @br_ricke @Esposito544Anna @BeateMermer @Francois7747 @Francois3064 @TomKnutson10 @zaratustra75 @KoesserSandra @carmen_borg60 @berg6bieu @wrigley_derrick @ArmellePrevot @Nikolay37836824 @Heike622 @AlineNathally3 @CaskettOlitz @luana_88_lima @MDegen55 I wish you a Good night Sharon, sleep well and have a magic weekend and happy Saturday. üíõüíô https://t.co/tRG0ARoCwg</td>
      <td id="T_dcd19_row1_col2" class="data row1 col2" >en</td>
    </tr>
    <tr>
      <th id="T_dcd19_level0_row2" class="row_heading level0 row2" >8144</th>
      <td id="T_dcd19_row2_col0" class="data row2 col0" >2021-07-24 07:04:46</td>
      <td id="T_dcd19_row2_col1" class="data row2 col1" >@Silke74918142 Guten Tag Silke ‚òïüôÇüåÑ</td>
      <td id="T_dcd19_row2_col2" class="data row2 col2" >de</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<section id="rule-based-lexicon-approaches">
<h2>Rule-Based/ Lexicon Approaches<a class="headerlink" href="#rule-based-lexicon-approaches" title="Permalink to this headline">#</a></h2>
<p>Lexicon-based approaches are still used in certain cases, especially when the goal is to perform sentiment analysis <strong>quickly and with a limited amount of computational resources</strong>. In lexicon-based sentiment analysis, words are assigned a sentiment score based on a pre-existing lexicon, such as SentiWordNet or the AFINN lexicon, and the overall sentiment of a text is calculated by summing the sentiment scores of the individual words.</p>
<p>Lexicon-based approaches have several advantages:</p>
<ul class="simple">
<li><p>Ease of use</p></li>
<li><p>Speed</p></li>
<li><p>Interpretability</p></li>
</ul>
<p>But there are downsides:</p>
<ul class="simple">
<li><p>Difficulties with sarcasm, negation, sentiment of words in context</p></li>
</ul>
<img src="https://t-redactyl.io/figure/Vader_1.jpg" align="right" width="20%">
<p><strong>Example lexicon</strong>: VADER (Valence Aware Dictionary and sEntiment Reasoner) <br> <br>
VADER is specifically attuned to sentiments expressed in social media. It uses a combination of sentiment-related words and emojis, along with some simple heuristics (punctuation, capitalisation, degree modifiers, conjuctions), to assign a sentiment score (positive, negative, or neutral) to a given piece of text. It‚Äôs output sentiment score is a numeric score between -1 and +1. The word sentiment scores range from -4 to 4 (neg to pos). <br> <br>
Example of some words in the lexicon and their scores: <br></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sentiment Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>beautiful</th>
      <td>2.9</td>
    </tr>
    <tr>
      <th>beautifuler</th>
      <td>2.1</td>
    </tr>
    <tr>
      <th>beautifulest</th>
      <td>2.6</td>
    </tr>
    <tr>
      <th>beautifully</th>
      <td>2.7</td>
    </tr>
    <tr>
      <th>beautifulness</th>
      <td>2.6</td>
    </tr>
    <tr>
      <th>beautify</th>
      <td>2.3</td>
    </tr>
    <tr>
      <th>beautifying</th>
      <td>2.3</td>
    </tr>
    <tr>
      <th>beauts</th>
      <td>1.7</td>
    </tr>
    <tr>
      <th>beauty</th>
      <td>2.8</td>
    </tr>
    <tr>
      <th>belittle</th>
      <td>-1.9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>A sample of how the VADER lexicon would classify the AIFER dataset (english tweets only):</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_61667 th {
  text-align: left;
}
#T_61667_row0_col0, #T_61667_row0_col1, #T_61667_row1_col0, #T_61667_row1_col1, #T_61667_row2_col0, #T_61667_row2_col1, #T_61667_row3_col0, #T_61667_row3_col1 {
  text-align: left;
}
</style>
<table id="T_61667">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_61667_level0_col0" class="col_heading level0 col0" >sentiment</th>
      <th id="T_61667_level0_col1" class="col_heading level0 col1" >text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_61667_level0_row0" class="row_heading level0 row0" >10089</th>
      <td id="T_61667_row0_col0" class="data row0 col0" >0.971600</td>
      <td id="T_61667_row0_col1" class="data row0 col1" >@Mini_okdoksmok @MariahCarey That has been THE year ü•∞ü•∞ü•∞</td>
    </tr>
    <tr>
      <th id="T_61667_level0_row1" class="row_heading level0 row1" >3646</th>
      <td id="T_61667_row1_col0" class="data row1 col0" >0.980600</td>
      <td id="T_61667_row1_col1" class="data row1 col1" >@MDegen55 Good morning my friend Gus. Enjoy the Tuesday. üòäüòäHave a beautiful Sunshine Day. üåûüçÄüåûüåûüç®üç®üçµüçµ https://t.co/40QEmvkC6Y</td>
    </tr>
    <tr>
      <th id="T_61667_level0_row2" class="row_heading level0 row2" >4045</th>
      <td id="T_61667_row2_col0" class="data row2 col0" >0.361200</td>
      <td id="T_61667_row2_col1" class="data row2 col1" >@jsneufeld @euro2021Athens Thank you, Janis</td>
    </tr>
    <tr>
      <th id="T_61667_level0_row3" class="row_heading level0 row3" >4659</th>
      <td id="T_61667_row3_col0" class="data row3 col0" >-0.773400</td>
      <td id="T_61667_row3_col1" class="data row3 col1" >My thoughts and prayers go out to the families and their friends who has lost somebody in this horrible flood üò¢üò¢üò¢üïä - and prayers for those, who are missing , they ll be back safe soon üñ§‚ù§Ô∏èüíõüá©üá™</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
</section>
<section id="machine-learning-approach-naive-bayes">
<h2>Machine Learning Approach: Naive Bayes<a class="headerlink" href="#machine-learning-approach-naive-bayes" title="Permalink to this headline">#</a></h2>
<p>Naive Bayes is a probabilistic algorithm (based on Bayes‚Äô theorem). It uses the probability of words or terms appearing in documents of different categories to determine the likelihood that a new document belongs to each category. It involves two basic steps:</p>
<blockquote>
<div><p><strong>Training</strong>: The algorithm learns the probability of words or terms appearing in each category. This is simply done by counting the number of occurrences of each word or term in the training corpus for each category and then computing the probability of each word or term given the category. This results in a set of word probabilities for each category.</p>
<p><strong>Classification</strong>: The algorithm uses the probabilities learned in the training step to classify new documents. For a new document, the algorithm calculates the likelihood of the document being in each category based on the probabilities of its words or terms. The category with the highest likelihood is chosen as the classification for the document.</p>
</div></blockquote>
<p>The ‚Äúnaive‚Äù aspect of Naive Bayes comes from the assumption that the occurrences of words or terms in a document are independent of one another, which is not always true(!).</p>
<section id="naive-bayes-functions">
<h3>Naive Bayes Functions<a class="headerlink" href="#naive-bayes-functions" title="Permalink to this headline">#</a></h3>
<p>Includes 3 functions:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">train_naive_bayes(freqs,</span> <span class="pre">train_x,</span> <span class="pre">train_y)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">naive_bayes_predict(tweet,</span> <span class="pre">logprior,</span> <span class="pre">loglikelihood)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_naive_bayes(test_x,</span> <span class="pre">test_y,</span> <span class="pre">logprior,</span> <span class="pre">loglikelihood,</span> <span class="pre">naive_bayes_predict=naive_bayes_predict)</span></code></p></li>
</ul>
<br>
<p><strong>Training Steps</strong></p>
<ul class="simple">
<li><p>Identify the number of classes (=2).</p></li>
<li><p>Create a probability for each class.
<span class="math notranslate nohighlight">\(P(D_{pos})\)</span> is the probability that given text/document is positive.
<span class="math notranslate nohighlight">\(P(D_{neg})\)</span> is the probability that the given text/document is negative.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P(D_{pos}) = \frac{D_{pos}}{D}\\]</div>
<div class="math notranslate nohighlight">
\[P(D_{neg}) = \frac{D_{neg}}{D}\\]</div>
<p>Where <span class="math notranslate nohighlight">\(D\)</span> is the total number of documents (or tweets), <span class="math notranslate nohighlight">\(D_{pos}\)</span> is the total number of positive tweets and <span class="math notranslate nohighlight">\(D_{neg}\)</span> is the total number of negative tweets.The probabilites are stored in a dict.</p>
<br>
<p><strong>Prior and Logprior</strong></p>
<p>The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the ‚Äúprior‚Äù.</p>
<p>The prior is the ratio of the probabilities <span class="math notranslate nohighlight">\(\frac{P(D_{pos})}{P(D_{neg})}\)</span>.
We can take the log of the prior to rescale it, and we‚Äôll call this the logprior</p>
<div class="math notranslate nohighlight">
\[
\text{logprior} = \log \left( \frac{P(D_{pos})}{P(D_{neg})} \right) = \log \left( \frac{D_{pos}}{D_{neg}} \right)
\]</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_naive_bayes</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Input:</span>
<span class="sd">        freqs: dictionary from (word, label) to how often the word appears</span>
<span class="sd">        train_x: a list of tweets</span>
<span class="sd">        train_y: a list of labels correponding to the tweets (0,1)</span>
<span class="sd">    Output:</span>
<span class="sd">        logprior: the log prior. (equation 3 above)</span>
<span class="sd">        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">loglikelihood</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">logprior</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># calculate V, the number of unique words in the vocabulary</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">freqs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>    

    <span class="c1"># calculate N_pos, N_neg, V_pos, V_neg</span>
    <span class="n">N_pos</span> <span class="o">=</span> <span class="n">N_neg</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">freqs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="c1"># if the label is positive (greater than zero)</span>
        <span class="k">if</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

            <span class="c1"># Increment the number of positive words by the count for this (word, label) pair</span>
            <span class="n">N_pos</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>

        <span class="c1"># else, the label is negative</span>
        <span class="k">else</span><span class="p">:</span>

            <span class="c1"># increment the number of negative words by the count for this (word,label) pair</span>
            <span class="n">N_neg</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
    
    <span class="c1"># Calculate D, the number of documents</span>
    <span class="n">D</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_y</span><span class="p">)</span>

    <span class="c1"># Calculate D_pos, the number of positive documents</span>
    <span class="n">D_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">train_y</span><span class="p">)</span>

    <span class="c1"># Calculate D_neg, the number of negative documents</span>
    <span class="n">D_neg</span> <span class="o">=</span> <span class="n">D</span> <span class="o">-</span> <span class="n">D_pos</span>

    <span class="c1"># Calculate logprior</span>
    <span class="n">logprior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">D_pos</span><span class="p">)</span> <span class="o">-</span>  <span class="n">np</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">D_neg</span><span class="p">)</span>
    
    <span class="c1"># For each word in the vocabulary...</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
        <span class="c1"># get the positive and negative frequency of the word</span>
        <span class="n">freq_pos</span> <span class="o">=</span> <span class="n">lookup</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">freq_neg</span> <span class="o">=</span> <span class="n">lookup</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># calculate the probability that each word is positive, and negative</span>
        <span class="n">p_w_pos</span> <span class="o">=</span> <span class="p">((</span><span class="n">freq_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_pos</span> <span class="o">+</span> <span class="n">V</span><span class="p">))</span>
        <span class="n">p_w_neg</span> <span class="o">=</span> <span class="p">((</span><span class="n">freq_neg</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_neg</span> <span class="o">+</span> <span class="n">V</span><span class="p">))</span>

        <span class="c1"># calculate the log likelihood of the word</span>
        <span class="n">loglikelihood</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_w_pos</span> <span class="o">/</span> <span class="n">p_w_neg</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">naive_bayes_predict</span><span class="p">(</span><span class="n">tweet</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Input:</span>
<span class="sd">        tweet: a string</span>
<span class="sd">        logprior: a number</span>
<span class="sd">        loglikelihood: a dictionary of words mapping to numbers</span>
<span class="sd">    Output:</span>
<span class="sd">        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)</span>

<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># process the tweet to get a list of words</span>
    <span class="n">word_l</span> <span class="o">=</span> <span class="n">process_tweet</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>

    <span class="c1"># initialize probability to zero</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># add the logprior</span>
    <span class="n">p</span> <span class="o">+=</span> <span class="n">logprior</span>

    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_l</span><span class="p">:</span>

        <span class="c1"># check if the word exists in the loglikelihood dictionary</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">loglikelihood</span><span class="p">:</span>
            <span class="c1"># add the log likelihood of that word to the probability</span>
            <span class="n">p</span> <span class="o">+=</span> <span class="n">loglikelihood</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_naive_bayes</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">,</span> <span class="n">naive_bayes_predict</span><span class="o">=</span><span class="n">naive_bayes_predict</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Input:</span>
<span class="sd">        test_x: A list of tweets</span>
<span class="sd">        test_y: the corresponding labels for the list of tweets</span>
<span class="sd">        logprior: the logprior</span>
<span class="sd">        loglikelihood: a dictionary with the loglikelihoods for each word</span>
<span class="sd">    Output:</span>
<span class="sd">        accuracy: (# of tweets classified correctly)/(total # of tweets)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># return this properly</span>

    <span class="n">y_hats</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">test_x</span><span class="p">:</span>
        <span class="c1"># if the prediction is &gt; 0</span>
        <span class="k">if</span> <span class="n">naive_bayes_predict</span><span class="p">(</span><span class="n">tweet</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># the predicted class is 1</span>
            <span class="n">y_hat_i</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># otherwise the predicted class is 0</span>
            <span class="n">y_hat_i</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># append the predicted class to the list y_hats</span>
        <span class="n">y_hats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_hat_i</span><span class="p">)</span>

    <span class="c1"># error is the average of the absolute values of the differences between y_hats and test_y </span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">y_hats</span> <span class="o">-</span> <span class="n">test_y</span><span class="p">))</span>

    <span class="c1"># Accuracy is 1 minus the error</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">error</span>

    <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
</div>
</div>
</div>
<br>
<p>Two datasets for training the Naive Bayes classifiers:
<br>
SemEval 2014 (Task 4)</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>polarity</th>
      <th>sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>But the staff was so horrible to us.</td>
      <td>service</td>
      <td>negative</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>To be completely fair, the only redeeming fact...</td>
      <td>food</td>
      <td>positive</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>To be completely fair, the only redeeming fact...</td>
      <td>anecdotes/miscellaneous</td>
      <td>negative</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>The food is uniformly exceptional, with a very...</td>
      <td>food</td>
      <td>positive</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Where Gabriela personaly greets you and recomm...</td>
      <td>service</td>
      <td>positive</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>‚Ä¶ and the Twemlab Goldstandard</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>label</th>
      <th>text</th>
      <th>goldstandard</th>
      <th>sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5</th>
      <td>200000000000000006</td>
      <td>sadness</td>
      <td>The unnamed woman, in her 30s, had been on the...</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>200000000000000007</td>
      <td>anger/disgust</td>
      <td>Attempt theft from Motor Vehicle - Rookery Par...</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>200000000000000008</td>
      <td>beauty</td>
      <td>Heather and webs... #heather #cobwebs #nature ...</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>200000000000000009</td>
      <td>sadness</td>
      <td>The unnamed woman, in her 30s, had been on the...</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>200000000000000010</td>
      <td>sadness</td>
      <td>Birmingham: Lianne Harris - mini Pype Hayes Pa...</td>
      <td>yes</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split both datasets into training and testing subsets and show overall sentiment distribution</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">test_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">split_test_train</span><span class="p">(</span><span class="n">semeval</span><span class="p">,</span> <span class="s2">&quot;SemEval&quot;</span><span class="p">)</span>
<span class="n">train_x_tw</span><span class="p">,</span> <span class="n">test_x_tw</span><span class="p">,</span> <span class="n">train_y_tw</span><span class="p">,</span> <span class="n">test_y_tw</span> <span class="o">=</span> <span class="n">split_test_train</span><span class="p">(</span><span class="n">twemlab</span><span class="p">,</span> <span class="s2">&quot;Twemlab&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Lexicon_and_Machine_Learning_22_0.png" src="_images/Lexicon_and_Machine_Learning_22_0.png" />
<img alt="_images/Lexicon_and_Machine_Learning_22_1.png" src="_images/Lexicon_and_Machine_Learning_22_1.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># for demo purposes, sample 5 english tweets</span>
<span class="n">sample_tweets</span> <span class="o">=</span> <span class="n">aifer_en</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># empty lists</span>
<span class="n">p_semeval</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">p_twemlab</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">tweets</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">numbers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">string_numbers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">numbers</span><span class="p">))</span>

<span class="c1"># get prediction from both NB classifiers for each tweet</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">sample_tweets</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">tweets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
    <span class="n">p_semeval</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">naive_bayes_predict</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">))</span>
    <span class="n">p_twemlab</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">naive_bayes_predict</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">logprior1</span><span class="p">,</span> <span class="n">loglikelihood1</span><span class="p">))</span>

<span class="c1"># function to plot results visually</span>
<span class="k">def</span> <span class="nf">plot_sentiments</span><span class="p">(</span><span class="n">numbers</span><span class="p">,</span> <span class="n">sentiments</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots a bar plot of the words and their sentiments.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">        words (list of str): A list of words.</span>
<span class="sd">        sentiments (list of float): A list of sentiment scores between -1 and 1 for each word.</span>
<span class="sd">        name (str): title of dataset</span>
<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#73E87F&#39;</span> <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">&#39;#F24E4E&#39;</span> <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">sentiments</span><span class="p">]</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">numbers</span><span class="p">,</span> <span class="n">sentiments</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Tweet Nr.&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Sentiments&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Naive Bayes Predictions based on </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> Training Data&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tweets</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">numbers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">tweets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># input list of tweets, the according sentiment scores, and the name of the dataset for NB training</span>
<span class="n">plot_sentiments</span><span class="p">(</span><span class="n">numbers</span><span class="p">,</span> <span class="n">p_semeval</span><span class="p">,</span> <span class="s2">&quot;SemEval&quot;</span><span class="p">)</span>
<span class="n">plot_sentiments</span><span class="p">(</span><span class="n">numbers</span><span class="p">,</span> <span class="n">p_twemlab</span><span class="p">,</span> <span class="s2">&quot;Twemlab&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1: @janlisiecki Do you know vegan ducklings, even bluish? https://t.co/LUdKqOydyD

2: @debbslovesnate @br_ricke @cheezysox @Firefly7113 @carmen_borg60 @homenor56 @zaratustra75 @karo8710 @anysogo @Esposito544Anna @stellapocecilia @bancroft_lotty @koshaid1 @Kranich65 @castlekitten47 @shannon_hausen @HartZsuzsanna @CaskettOlitz @caskettuniverse @NathanFillion @NFillionOnline @germannateclub @castle_all @MononenMinna @Star_Medd @pocs80 @CheerfulChemist @castle_alwayys @MDegen55 Have a great weekend Debbie. Enjoy the afternoon. üçµüåûüíû https://t.co/LkjCcJTIou

3: Explored Cochem Castle and the Mosel River Valley yesterday. https://t.co/1wkfMW7aR1

4: @dpd_de In March I received late a parcel &amp; I had to throw part of its content away bc it was rotten. Today I had to receive another parcel &amp; the delivery man didn‚Äôt follow my instructions, I will get it 3 days late. I‚Äôll have to throw away part of the content AGAIN. No words‚Ä¶

5: @MDegen55 Good morning @LawrenCali @leingh @moa_roos @cibir68 @elena_cavallini @iges2u @crstanakatic @christeld23 @Kranich65 @bestofstana @RobertVallet @maricarmengar15 @RossettaStonex @fandelmejor684 @andibeth012 @19Skorpion60 @Maren2410 @manymoonstoo @Carmen21272 @APRN1119 https://t.co/u9L36b6Rfv
</pre></div>
</div>
<img alt="_images/Lexicon_and_Machine_Learning_25_1.png" src="_images/Lexicon_and_Machine_Learning_25_1.png" />
<img alt="_images/Lexicon_and_Machine_Learning_25_2.png" src="_images/Lexicon_and_Machine_Learning_25_2.png" />
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Naive Bayes accuracy when trained on SemEval 2014 = 0.7947
Naive Bayes accuracy when trained on Twemlab = 0.9121
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="machine-learning-approach-support-vector-machine">
<h2>Machine Learning Approach: Support Vector Machine<a class="headerlink" href="#machine-learning-approach-support-vector-machine" title="Permalink to this headline">#</a></h2>
<img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_svm_margin_thumb.png" align="right"> 
<p>Using readily available libraries like sklearn, an SVM classifier (in this case SVC (Support Vector Classification)) can be trained for binary classification problems like pos-neg sentiment classification. The SVC classifier predicts the class label of a given sample based on the feature set. It solves the optimization problem to find a hyperplane that maximally separates the positive and negative class samples in the feature space. The optimization problem is solved using the maximum margin principle, where the margin is the distance between the hyperplane and the closest samples from each class, called support vectors. The SVC classifier is a useful tool for performing binary classification problems, particularly when the number of features is high, and the data is not linearly separable.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import libraries</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>

<span class="c1"># semeval binary sentimetns pos-neg</span>
<span class="n">semeval_binary</span> <span class="o">=</span> <span class="n">semeval</span><span class="p">[(</span><span class="n">semeval</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">semeval</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)]</span>

<span class="c1"># Convert the text data into numerical features using CountVectorizer</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>

<span class="c1"># Preprocess and split the data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">semeval_binary</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">semeval_binary</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span> <span class="c1"># labels</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Train the classifier</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Test the classifier</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate the performance</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy: &#39;</span><span class="p">,</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Precision: &#39;</span><span class="p">,</span> <span class="n">precision</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Recall: &#39;</span><span class="p">,</span> <span class="n">recall</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;F1-score: &#39;</span><span class="p">,</span> <span class="n">f1</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy:  0.762
Precision:  0.772
Recall:  0.939
F1-score:  0.847
</pre></div>
</div>
</div>
</div>
<p>Using the trained (on SemEval 2014) SVM classifier to classify the AIFER english dataset. Here is a sample of the classification:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_69cf0 th {
  text-align: left;
}
#T_69cf0_row0_col0, #T_69cf0_row0_col1, #T_69cf0_row1_col0, #T_69cf0_row1_col1, #T_69cf0_row2_col0, #T_69cf0_row2_col1, #T_69cf0_row3_col0, #T_69cf0_row3_col1, #T_69cf0_row4_col0, #T_69cf0_row4_col1 {
  text-align: left;
}
</style>
<table id="T_69cf0">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_69cf0_level0_col0" class="col_heading level0 col0" >SVM Sentiment</th>
      <th id="T_69cf0_level0_col1" class="col_heading level0 col1" >Text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_69cf0_level0_row0" class="row_heading level0 row0" >10550</th>
      <td id="T_69cf0_row0_col0" class="data row0 col0" >1</td>
      <td id="T_69cf0_row0_col1" class="data row0 col1" >Explored Cochem Castle and the Mosel River Valley yesterday. https://t.co/1wkfMW7aR1</td>
    </tr>
    <tr>
      <th id="T_69cf0_level0_row1" class="row_heading level0 row1" >1840</th>
      <td id="T_69cf0_row1_col0" class="data row1 col0" >1</td>
      <td id="T_69cf0_row1_col1" class="data row1 col1" >@MDegen55 Happy Wednesday Debbie. üíõüíôüíö https://t.co/s71wEY6LZz</td>
    </tr>
    <tr>
      <th id="T_69cf0_level0_row2" class="row_heading level0 row2" >510</th>
      <td id="T_69cf0_row2_col0" class="data row2 col0" >1</td>
      <td id="T_69cf0_row2_col1" class="data row2 col1" >@koshaid1 @pocs80 @AnetteRuff1 @piaroos1 @moa_roos @Jenny_S3005 @nlopes952 @MononenMinna @sadino22 @blovencik @HartZsuzsanna @FabischKerstin @stellapocecilia @MontrucchiPaola @Chrissychatt @Kranich65 @iges2u @frmirielis @andibeth012 @christeld23 @manfred_degen @60Ritschi @MDegen55 Good evening Gus. https://t.co/rg4ckBWmiT</td>
    </tr>
    <tr>
      <th id="T_69cf0_level0_row3" class="row_heading level0 row3" >94</th>
      <td id="T_69cf0_row3_col0" class="data row3 col0" >0</td>
      <td id="T_69cf0_row3_col1" class="data row3 col1" >@KalengaKamwendo Bro till this day, I am mad my dad asked me to focus on school & not Sport.üòí I think we need better systems to identify & develop talent.</td>
    </tr>
    <tr>
      <th id="T_69cf0_level0_row4" class="row_heading level0 row4" >852</th>
      <td id="T_69cf0_row4_col0" class="data row4 col0" >1</td>
      <td id="T_69cf0_row4_col1" class="data row4 col1" >you don‚Äôt 
always need a plan - 
sometimes you 
just need to breathe,
trust, let go,
and see what happens.
‚òÜ

SHOOTING. SCHLOSS KOBLENZ
photo. @le_fotografie_cgn 

#danielagviolin #danielagcom #violin #violinist #geige #violinshoot #fotoshoot #schlosskoblenz #koblenz https://t.co/EpbL1113LK</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/Lexicon_and_Machine_Learning_32_0.png" src="_images/Lexicon_and_Machine_Learning_32_0.png" />
</div>
</div>
<p>In recent years, the use of lexicon-based methods and traditional machine learning models for sentiment analysis has <strong>decreased in popularity</strong>. This shift has been largely driven by the <strong>emergence of deep learning models</strong>, which have shown impressive results on NLP tasks such as sentiment classification. With the availability of vast amounts of text data, deep learning models such as transformers have been able to capture more complex patterns and relationships within the data, leading to improved performance compared to lexicon-based and machine learning models. As a result, deep learning methods are now seen as the state-of-the-art approach for sentiment analysis and other NLP tasks.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Introduction_to_SA.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Introduction to Sentiment Analysis</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Deep_Learning.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Document Level Sentiment Analysis with Deep Learning Models</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book Community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>